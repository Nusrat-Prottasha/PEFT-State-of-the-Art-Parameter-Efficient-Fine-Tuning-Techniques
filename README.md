<h1 align="center">‚ú® Awesome PEFT A2Z </em> ‚ú®</h1>
<h2 align="center"> PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models </em> </h2>

<p align="center">
  <a href="https://github.com/Nusrat-Prottasha/PEFT-A2Z">
    <img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" alt="Awesome Badge">
  </a>
</p>
<p align="center">

  <img src="https://img.shields.io/github/stars/Nusrat-Prottasha/PEFT-State-of-the-Art-Parameter-Efficient-Fine-Tuning-Techniques?color=orange&style=for-the-badge" alt="GitHub stars">
  <img src="https://img.shields.io/github/forks/Nusrat-Prottasha/PEFT-State-of-the-Art-Parameter-Efficient-Fine-Tuning-Techniques?color=blueviolet&style=for-the-badge" alt="GitHub forks">
  <img src="https://img.shields.io/github/last-commit/Nusrat-Prottasha/PEFT-State-of-the-Art-Parameter-Efficient-Fine-Tuning-Techniques?color=green&style=for-the-badge" alt="GitHub last commit">
  <img src="https://img.shields.io/github/issues/Nusrat-Prottasha/PEFT-State-of-the-Art-Parameter-Efficient-Fine-Tuning-Techniques?color=crimson&style=for-the-badge" alt="GitHub issues">
  
  <a href="https://GitHub.com/Nusrat-Prottasha/PEFT-State-of-the-Art-Parameter-Efficient-Fine-Tuning-Techniques/graphs/commit-activity">
    <img src="https://img.shields.io/badge/Maintained%3F-yes-brightgreen.svg?style=for-the-badge" alt="Maintenance">
  </a>

</p>

<p align="center">
    <img src="https://i.imgur.com/waxVImv.png" alt="Oryx Video-ChatGPT">
</p>

<p align="center">

<img src="./figures/peft_tree.drawio.png" width="870">

</p>

## üöÄ <span id="head1"> *Introduction* </span>

Large Language Models (LLMs) and Pre‚Äëtrained Language Models (PLMs) have revolutionized artificial intelligence, driving breakthroughs in text generation, translation, conversational agents, and multimodal understanding. However, their sheer scale‚Äîoften billions of parameters‚Äîmakes full fine‚Äëtuning prohibitively expensive in compute, memory, and storage. This work addresses that gap by surveying **Parameter‚ÄëEfficient Fine‚ÄëTuning (PEFT)** methods, which adapt large models by updating only a small fraction of parameters. We catalog foundational techniques (e.g., adapters, prefix‚Äëtuning, LoRA), present a unified taxonomy, and identify open challenges and future directions for making large‚Äëscale model adaptation more accessible and sustainable.



## üí° <span id="head1"> *Abstract* </span>

Large models such as Large Language Models (LLMs) and Vision Language Models (VLMs) have transformed artificial intelligence, powering applications in natural language processing, computer vision, and multimodal learning. However, fully fine-tuning these models remains expensive, requiring extensive computational resources, memory, and task-specific data. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a promising solution that allows adapting large models to downstream tasks by updating only a small portion of parameters. This survey presents a comprehensive overview of PEFT techniques, focusing on their motivations, design principles, and effectiveness. We begin by analyzing the resource and accessibility challenges posed by traditional fine-tuning and highlight key issues, such as overfitting, catastrophic forgetting, and parameter inefficiency. We then introduce a structured taxonomy of PEFT methods‚Äîgrouped into additive, selective, reparameterized, hybrid, and unified frameworks‚Äîand systematically compare their mechanisms and trade-offs. Beyond taxonomy, we explore the impact of PEFT across diverse domains, including language, vision, and generative modeling, showing how these techniques offer strong performance with lower resource costs. We also discuss important open challenges in scalability, interpretability, and robustness, and suggest future directions such as federated learning, domain adaptation, and theoretical grounding. Our goal is to provide a unified understanding of PEFT and its growing role in enabling practical, efficient, and sustainable use of large models.

## ‚≠ê <span id="head1"> *Citation* </span>

If you find this useful for your research, please cite it below:

## üöÄ Serial Adapters

- Parameter-Efficient Transfer Learning for NLP [[Paper](https://arxiv.org/abs/1902.00751)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/google-research/adapter-bert)

- AdapterHub: A Framework for Adapting Transformers [[Paper](https://arxiv.org/abs/2007.07779)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/adapterhub-a-framework-for-adapting)

- MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer [[Paper](https://arxiv.org/abs/2005.00052)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/mad-x-an-adapter-based-framework-for-multi)

- Cross-Lingual Transfer with Target Language-Ready Task Adapters [[Paper](https://aclanthology.org/2023.findings-acl.13.pdf)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/parovicm/tlr-adapters?tab=readme-ov-file)

- BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning [[Paper](https://arxiv.org/pdf/1902.02671)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/AsaCooperStickland/Bert-n-Pals)



## üöÄ Parallel Adapters

- UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling [[Paper](https://arxiv.org/pdf/2302.06605)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/UniAdapter/UniAdapter)

- UniPT: Universal Parallel Tuning for Transfer Learning with Efficient Parameter and Memory [[Paper](https://arxiv.org/pdf/2308.14316)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/Paranioar/UniPT)

- AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition [[Paper](https://arxiv.org/pdf/2205.13535)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/ShoufaChen/AdaptFormer)

- BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning [[Paper](https://arxiv.org/pdf/1902.02671)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/AsaCooperStickland/Bert-n-Pals?tab=readme-ov-file)

- PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning [[Paper](https://arxiv.org/pdf/2402.15082)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/JachinLin2022/PEMT)

- Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference [[Paper](https://arxiv.org/pdf/2304.04947)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/conditional-adapters-parameter-efficient)


## üöÄ Hybrid Adapters

- AUTOPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning [[Paper](https://arxiv.org/pdf/2301.12132)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/cambridgeltl/autopeft)

- CROSS-MODAL ADAPTER: PARAMETER-EFFICIENT TRANSFER LEARNING APPROACH FOR VISION-LANGUAGE MODELS [[Paper](https://arxiv.org/pdf/2404.12588)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/cross-modal-adapter-parameter-efficient)

- EFFICIENT REMOTE SENSING WITH HARMONIZED TRANSFER LEARNING AND MODALITY ALIGNMENT [[Paper](https://arxiv.org/pdf/2404.18253)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/seekerhuang/HarMA?tab=readme-ov-file)

- MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval [[Paper](https://arxiv.org/pdf/2301.07868)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/zhangbw17/MV-Adapter)

- Conv-Adapter: Exploring Parameter Efficient Transfer Learning for ConvNets [[Paper](https://arxiv.org/pdf/2208.07463)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/Hhhhhhao/Conv-Adapter)

- Conditional Adapters: Parameter-Efficient Transfer Learning with Fast Inference [[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/19d7204af519eae9993f7f72377a0ec0-Paper-Conference.pdf)]
  ![NeurIPS](https://img.shields.io/badge/NeurIPS-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/conditional-adapters-parameter-efficient)


## üöÄ Single Task

- VISION TRANSFORMER ADAPTER FOR DENSE PREDICTIONS [[Paper](https://arxiv.org/pdf/2205.08534)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/czczup/ViT-Adapter)

- Simple, Scalable Adaptation for Neural Machine Translation [[Paper](https://aclanthology.org/D19-1165.pdf)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/simple-scalable-adaptation-for-neural-machine)

- K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters [[Paper](https://arxiv.org/pdf/2002.01808)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/microsoft/K-Adapter)


## üöÄ Multi Task

- K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters [[Paper](https://arxiv.org/pdf/2002.01808)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/microsoft/K-Adapter)

- AdapterFusion: Non-Destructive Task Composition for Transfer Learning [[Paper](https://arxiv.org/pdf/2005.00247)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/adapterfusion-non-destructive-task)

- OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy [[Paper](https://arxiv.org/pdf/2401.10559)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- Multi-Head Adapter Routing for Cross-Task Generalization [[Paper](https://arxiv.org/pdf/2211.03831)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/microsoft/mttl)

- Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks [[Paper](https://arxiv.org/pdf/2106.04489)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/rabeehk/hyperformer)

- When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications [[Paper](https://arxiv.org/pdf/2310.18339)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/liuqidong07/MOELoRA-peft)

- LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models [[Paper](https://aclanthology.org/2023.emnlp-main.319.pdf)]
  ![arXiv](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/AGI-Edgerunners/LLM-Adapters)

- AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models [[Paper](https://arxiv.org/pdf/2302.07027)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/adaptersoup-weight-averaging-to-improve)


## üöÄ Continuous Prompting 

- Prefix-Tuning: Optimizing Continuous Prompts for Generation [[Paper](https://arxiv.org/pdf/2101.00190)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/XiangLi1999/PrefixTuning)

- PEDRO: Parameter-Efficient Fine-tuning with Prompt DEpenDent Representation MOdification [[Paper](https://www.arxiv.org/pdf/2409.17834)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- DEPT: Decomposed Prompt Tuning for Parameter-Efficient Fine-Tuning [[Paper](https://arxiv.org/pdf/2309.05173)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/ZhengxiangShi/DePT)

- P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks [[Paper](https://arxiv.org/pdf/2110.07602)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/THUDM/P-tuning-v2)

- Q-PEFT: Query-dependent Parameter Efficient Fine-tuning for Text Reranking with Large Language Models [[Paper](https://arxiv.org/pdf/2404.04522)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- PTR: Prompt Tuning with Rules for Text Classification [[Paper](https://arxiv.org/pdf/2105.11259)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/thunlp/PTR)

- Prefix-Propagation: Parameter-Efficient Tuning for Long Sequences [[Paper](https://web3.arxiv.org/pdf/2305.12086)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/MonliH/prefix-propagation)

- Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts [[Paper](https://arxiv.org/pdf/2210.11292)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/xyltt/LPT)

- OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning [[Paper](https://arxiv.org/pdf/2402.04129)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jpmorganchase/ovor)


## üöÄ Discrete Prompt

- RLPROMPT: Optimizing Discrete Text Prompts with Reinforcement Learning [[Paper](https://arxiv.org/pdf/2205.12548)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/mingkaid/rl-prompt)

- SPARSEFIT: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations [[Paper](https://arxiv.org/pdf/2305.13235)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/AkihikoWatanabe/paper_notes/issues/1684)

- OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning [[Paper](https://arxiv.org/pdf/2402.04129)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jpmorganchase/ovor)

  


## üöÄ Domain Specific Adaption (Natural Language Understanding)

- Prompt Tuning Strikes Back: Customizing Foundation Models with Low-Rank Prompt Adaptation [[Paper](https://arxiv.org/pdf/2405.15282)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jabhinav/Prompt-Tuning-Strikes-Back-with-LOPA)

- InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding [[Paper](https://arxiv.org/pdf/2306.04933)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/infoprompt-information-theoretic-soft-prompt)

- PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization [[Paper](https://arxiv.org/pdf/2407.18078)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/ChrisIsKing/Parameter-Efficient-Personalization)

- IDPG: An Instance-Dependent Prompt Generation Method [[Paper](https://arxiv.org/pdf/2204.04497)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/CSerxy/IDPG)

- APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models [[Paper](https://aclanthology.org/2023.emnlp-main.567.pdf)]
  ![EMNLP](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/wgcban/apt)

- SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts [[Paper](https://aclanthology.org/2023.emnlp-main.884.pdf)]
  ![EMNLP](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jyjohnchoi/SMoP)



## üöÄ Task Specific Adaption

- The Power of Scale for Parameter-Efficient Prompt Tuning [[Paper](https://arxiv.org/pdf/2104.08691)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/mkshing/Prompt-Tuning)

- SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer [[Paper](https://arxiv.org/pdf/2110.07904)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/spot-better-frozen-model-adaptation-through)

- APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference [[Paper](https://arxiv.org/html/2401.12200v2)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- XPROMPT: Exploring the Extreme of Prompt Tuning [[Paper](https://arxiv.org/pdf/2210.04457)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/BD-MF/XPrompt?tab=readme-ov-file)

- Parameter Efficient Multi-task Fine-tuning by Learning to Transfer Token-wise Prompts [[Paper](https://aclanthology.org/2023.findings-emnlp.584.pdf)]
  ![EMNLP](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/mlwu22/TPT)

- IDPG: An Instance-Dependent Prompt Generation Method [[Paper](https://arxiv.org/pdf/2204.04497)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/CSerxy/IDPG)

- APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models [[Paper](https://aclanthology.org/2023.emnlp-main.567.pdf)]
  ![EMNLP](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/wgcban/apt)

- SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts [[Paper](https://aclanthology.org/2023.emnlp-main.884.pdf)]
  ![EMNLP](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jyjohnchoi/SMoP)




## üöÄ Scaling Adaption

- Propulsion: Steering LLM with Tiny Fine-Tuning [[Paper](https://arxiv.org/pdf/2409.10927)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/Kowsher/Propulsion)


## üöÄ Selective Tuning Based on Parameter Importance

- Parameter-Efficient Transfer Learning with Diff Pruning [[Paper](https://arxiv.org/pdf/2012.07463)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/dguo98/DiffPruning)

- Targeted Efficient Fine-tuning: Optimizing Parameter Updates with Data-Driven Sample Selection [[Paper](https://arxiv.org/pdf/2403.08484)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models [[Paper](https://arxiv.org/pdf/2410.11772)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/Kaiseem/IST)

- AdaFish: Fast Low-Rank Parameter-Efficient Fine-Tuning by Using Second-Order Information [[Paper](https://arxiv.org/pdf/2403.13128v1)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/adafish-fast-low-rank-parameter-efficient)

- Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning [[Paper](https://arxiv.org/pdf/2311.03748)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/psunlpgroup/FISH-DIP)



## üöÄ Unstructured Mask

- Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models [[Paper](https://arxiv.org/pdf/2305.16597)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/neural-architecture-search-for-parameter)

- Composable Sparse Fine-Tuning for Cross-Lingual Transfer [[Paper](https://arxiv.org/pdf/2110.07560)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/cambridgeltl/composable-sft)

- Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning [[Paper](https://arxiv.org/pdf/2109.05687)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/RunxinXu/ChildTuning)

- Parameter-Efficient Fine-Tuning without Introducing New Latency [[Paper](https://arxiv.org/pdf/2305.16742)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/parameter-efficient-fine-tuning-without)



## üöÄ Structured Mask


- Efficient Fine-Tuning of BERT Models on the Edge [[Paper](https://arxiv.org/pdf/2205.01541)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/efficient-fine-tuning-of-bert-models-on-the)

- Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation [[Paper](https://arxiv.org/pdf/2104.08771)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/MGheini/xattn-transfer-for-mt)

- X-PEFT: eXtremely Parameter-Efficient Fine-Tuning for Extreme Multi-Profile Scenarios [[Paper](https://arxiv.org/pdf/2401.16137)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- Structured Unrestricted-Rank Matrices for Parameter Efficient Fine-tuning [[Paper](https://ar5iv.labs.arxiv.org/html/2406.17740)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/arijitthegame/structured-matrices-PEFT)


## üöÄ Core Low Rank Decomposition


- LoRA: Low-Rank Adaptation of Large Language Models [[Paper](https://arxiv.org/pdf/2106.09685)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/microsoft/LoRA)

- Compacter: Efficient Low-Rank Hypercomplex Adapter Layers [[Paper](https://arxiv.org/pdf/2106.04647)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/rabeehk/compacter)

- Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning [[Paper](https://arxiv.org/pdf/2012.13255)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/intrinsic-dimensionality-explains-the)

- Parameter-Efficient Model Adaptation for Vision Transformers [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/25160)]
  ![AAAI](https://img.shields.io/badge/AAAI-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)

- Parameter-Efficient Fine-Tuning without Introducing New Latency [[Paper](https://arxiv.org/pdf/2305.16742)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/parameter-efficient-fine-tuning-without)

- DoRA: Weight-Decomposed Low-Rank Adaptation [[Paper](https://arxiv.org/pdf/2402.09353)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/NVlabs/DoRA)

- LLMEmbed: Rethinking Lightweight LLM‚Äôs Genuine Function in Text Classification [[Paper](https://arxiv.org/pdf/2406.03725)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/ChunLiu-cs/LLMEmbed-ACL2024)



## üöÄ Adaptive and dynamic rank methods

- DyLoRA: Parameter-Efficient Tuning of Pretrained Models using Dynamic Search-Free Low Rank Adaptation [[Paper](https://arxiv.org/pdf/2210.07558)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/dylora-parameter-efficient-tuning-of-pre)

- AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning [[Paper](https://arxiv.org/pdf/2303.10512)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/QingruZhang/AdaLoRA)

- Sparse Low-Rank Adaptation of Pre-trained Language Models [[Paper](https://arxiv.org/pdf/2311.11696)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/TsinghuaC3I/SoRA)

- Increasing Model Capacity for Free: A Simple Strategy for Parameter-Efficient Fine-Tuning [[Paper](https://arxiv.org/pdf/2407.01320)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/LINs-lab/CapaBoost)

- AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning [[Paper](https://arxiv.org/pdf/2403.09113)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://anonymous.4open.science/r/AutoLoRA)



## üöÄ Enhanced LoRA variants for fine tuning efficiency

- Bayesian Low-Rank Adaptation for Large Language Models [[Paper](https://arxiv.org/html/2308.13111v5)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/MaximeRobeyns/bayesian_lora)

- LoRA Dropout as a Sparsity Regularizer for Overfitting Control [[Paper](https://arxiv.org/html/2404.09610v1)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/lora-dropout-as-a-sparsity-regularizer-for)

- PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization [[Paper](https://arxiv.org/pdf/2402.16141)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/periodiclora-breaking-the-low-rank-bottleneck)

- LoRA+: Efficient Low Rank Adaptation of Large Models [[Paper](https://arxiv.org/pdf/2402.12354)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/microsoft/LoRA)

- Mixture-of-Subspaces in Low-Rank Adaptation [[Paper](https://arxiv.org/pdf/2406.11909)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/wutaiqiang/MoSLoRA)

- Continual Learning with Low Rank Adaptation [[Paper](https://arxiv.org/pdf/2311.17601)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/continual-learning-with-low-rank-adaptation)

- Trans-LoRA: Towards Data-Free Transferable Parameter Efficient Finetuning [[Paper](https://arxiv.org/html/2405.17258v1)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/textit-trans-lora-towards-data-free)

- RoseLoRA: Row and Column-wise Sparse Low-Rank Adaptation [[Paper](https://arxiv.org/pdf/2406.10777)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/lliutianc/roselora)

- Low-Rank Few-Shot Adaptation of Vision-Language Models [[Paper](https://arxiv.org/html/2405.18541v2)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/MaxZanella/CLIP-LoRA)

- SVDQUANT: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models [[Paper](https://arxiv.org/pdf/2411.05007)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file)

- Variational Low-Rank Adaptation Using IVON [[Paper](https://arxiv.org/pdf/2411.04421)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/team-approx-bayes/ivon-lora)

- PMoL: Parameter Efficient MoE for Preference Mixing of LLM Alignment [[Paper](https://arxiv.org/pdf/2411.01245)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/pmol-parameter-efficient-moe-for-preference)

- Empower Vision Applications with LoRA LMM [[Paper](https://arxiv.org/pdf/2411.00915)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition [[Paper](https://arxiv.org/pdf/2307.13269)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/sail-sg/lorahub)

- MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications [[Paper](https://arxiv.org/pdf/2310.18339v1)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/liuqidong07/MOELoRA-peft)

- Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning [[Paper](https://arxiv.org/pdf/2309.05444)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/for-ai/parameter-efficient-moe)

- Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models [[Paper](https://arxiv.org/html/2403.03432v1)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/mixture-of-loras-an-efficient-multitask)

- MIXTURE OF LORA EXPERTS [[Paper](https://arxiv.org/pdf/2404.13628)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based Mixture of Experts [[Paper](https://arxiv.org/html/2404.15159v2)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/TUDB-Labs/MixLoRA)



## üöÄ Hybrid Approaches 


- Towards a Unified View of Parameter-Efficient Transfer Learning [[Paper](https://arxiv.org/pdf/2110.04366)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jxhe/unify-parameter-efficient-tuning?tab=readme-ov-file)

- UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning [[Paper](https://arxiv.org/pdf/2110.07577)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/morningmoni/UniPELT)

- Parameter-Efficient Fine-Tuning Design Spaces [[Paper](https://arxiv.org/pdf/2301.01821)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/amazon-science/peft-design-spaces)

- Neural Prompt Search [[Paper](https://arxiv.org/pdf/2206.04673)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/ZhangYuanhan-AI/NOAH)

- AUTOPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning [[Paper](https://arxiv.org/pdf/2301.12132)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/cambridgeltl/autopeft)

- LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models [[Paper](https://arxiv.org/pdf/2304.01933)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/AGI-Edgerunners/LLM-Adapters)

- RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation [[Paper](https://arxiv.org/pdf/2401.04679)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/IST-DASLab/RoSA)

- Sparsity- and Hybridity-Inspired Visual Parameter-Efficient Fine-Tuning for Medical Diagnosis [[Paper](https://arxiv.org/pdf/2405.17877)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks [[Paper](https://arxiv.org/pdf/2203.03878)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/hyperpelt-unified-parameter-efficient)

- Hydra: Multi-head Low-rank Adaptation for Parameter Efficient Fine-tuning [[Paper](https://arxiv.org/pdf/2309.06922)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/extremebird/Hydra)


## üöÄ MoE Based

- When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications [[Paper](https://arxiv.org/pdf/2310.18339)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/liuqidong07/MOELoRA-peft)

- MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based Mixture of Experts [[Paper](https://arxiv.org/pdf/2404.15159)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/TUDB-Labs/MixLoRA)

- Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning [[Paper](https://arxiv.org/pdf/2309.05444)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/for-ai/parameter-efficient-moe)

- Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models [[Paper](https://arxiv.org/pdf/2403.03432)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://gist.github.com/ruvnet/809d0312c1c599ba29721c93a20a741c)

- Mixture-of-Subspaces in Low-Rank Adaptation [[Paper](https://arxiv.org/pdf/2406.11909)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/wutaiqiang/MoSLoRA)


# PEFT Methods in NLP Tasks

## Models and Papers

| Year | Model | Paper Title | Links |
|---------|-------|-------------|-------|
| 15 Oct 2024 | Bloomz | Investigating translation for Indic languages with BLOOMZ-3b through prompting and LoRA fine-tuning | [Paper](https://www.nature.com/articles/s41598-024-74617-9) |
| 31 July 2024 | LLaMA3-8B | The Llama 3 Herd of Models | [arXiv](https://arxiv.org/abs/2407.21783) [GitHub](https://github.com/meta-llama/llama3) |
| 18 Aug 2024 | Recorder | A Syntax-Guided Edit Decoder for Neural Program Repair | [Paper](https://dl.acm.org/doi/10.1145/3468264.3468544) |
| 18 June 2024 | ChatGLM-6B | ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools | [arXiv](https://arxiv.org/abs/2406.12793) [GitHub](https://github.com/THUDM/ChatGLM-6B) |
| 26 June 2024 | Tag-LLaMA | Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains | [arXiv](https://arxiv.org/abs/2402.05140) |
| 31 May 2024 | Mamba-2 | Mamba: Linear-Time Sequence Modeling with Selective State Spaces | [arXiv](https://arxiv.org/abs/2405.21060) [GitHub](https://github.com/state-spaces/mamba) |
| 15 May 2024 | LLaVA-1.5 | Improved Baselines with Visual Instruction Tuning | [arXiv](https://arxiv.org/abs/2310.03744) [GitHub](https://github.com/haotian-liu/LLaVA) |
| 16 April 2024 | GEMMA-2B, Gemma-7B | Gemma: Open Models Based on Gemini Research and Technology | [arXiv](https://arxiv.org/abs/2403.08295) [GitHub](https://github.com/google-gemma/gemma) |
| 16 Mar 2024 | Openchat8B | OpenChat: Advancing Open-source Language Models with Mixed-Quality Data | [arXiv](https://arxiv.org/abs/2309.11235) [GitHub](https://github.com/imoneoi/openchat) |
| 26 Feb 2024 | Airavata-7b | Airavata: Introducing Hindi Instruction-tuned LLM | [arXiv](https://arxiv.org/html/2401.15006v2) |
| 14 Feb 2024 | LlaSMol | LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset | [arXiv](https://arxiv.org/abs/2402.09391) [GitHub](https://github.com/OSU-NLP-Group/LLM4Chem) |
| 4 Jan 2024 | TinyLlama | TinyLlama: An Open-Source Small Language Model | [arXiv](https://arxiv.org/abs/2401.02385) [GitHub](https://github.com/jzhang38/TinyLlama) |
| 2024 | DeepSeek-Coder-Base-6.7B | DeepSeek Coder: When the Large Language Model Meets Programming | [arXiv](https://arxiv.org/abs/2401.14196) [GitHub](https://github.com/deepseek-ai/DeepSeek-Coder) |
| 2024 | GPT-J (6B) | GPT-J: 6B Parameter Open Source Transformer Model | [GitHub](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/GPT-J-6B) |
| 14 Dec 2024 | TigerBot-7B | TigerBot: A Multi-stage Open-Source LLM for Human and Agent Scenarios | [arXiv](https://arxiv.org/abs/2312.08688) [GitHub](https://github.com/TigerResearch/TigerBot) |
| 30 Nov 2023 | CCT5 | A Code-Change-Oriented Pre-trained Model | [Paper](https://dl.acm.org/doi/abs/10.1145/3611643.3616339) |
| 29 Nov 2023 | Falcon | The Falcon Series of Open Language Models | [arXiv](https://arxiv.org/pdf/2311.16867) |
| 21 Nov 2023 | ShareGPTv4(7B) | ShareGPT4V: Improving Large Multi-Modal Models with Better Captions | [arXiv](https://arxiv.org/abs/2311.12793) [GitHub](https://sharegpt4v.github.io/) |
| Dec 1 2023 | Mamba | Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality | [arXiv](https://arxiv.org/abs/2312.00752) [GitHub](https://github.com/state-spaces/mamba) |
| 10 Oct 2023 | Mistral-7B-Instruct | Mistral 7B | [arXiv](https://arxiv.org/abs/2310.06825) [GitHub](https://github.com/mistralai/mistral-inference) |
| 24 Aug 2023 | Qwen-VL-Chat(7B) | Qwen-VL: A Vision-Language Foundation Model for Universal Multimodal Understanding and Generation | [arXiv](https://arxiv.org/abs/2308.12966) [GitHub](https://github.com/QwenLM/Qwen-VL) |
| 24 Aug 2023 | CodeLlama-7B | Code Llama: Open Foundation Models for Code | [arXiv](https://arxiv.org/abs/2308.12950) [GitHub](https://github.com/facebookresearch/codellama) |
| 18 July 2023 | Llama2-7b,3b,13b,70b | Llama 2: Open Foundation and Fine-Tuned Chat Models | [arXiv](https://arxiv.org/abs/2307.09288) [GitHub](https://github.com/facebookresearch/llama) |
| 27 June 2023 | BLOOM-7B,1B | BLOOM: A 176B-Parameter Open-Access Multilingual Language Model | [arXiv](https://arxiv.org/abs/2211.05100) [GitHub](https://github.com/bigscience-workshop/multilingual-modeling) |
| 9 June 2023 | Vicuna-7b,13b | Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena | [arXiv](https://arxiv.org/abs/2306.05685) [GitHub](https://github.com/lm-sys/FastChat) |
| 15 Mar 2023 | GPT-4 | GPT-4 Technical Report | [arXiv](https://arxiv.org/abs/2303.08774) |
| 27 Feb 2023 | LLaMA30B | LLaMA: Open and Efficient Foundation Language Models | [arXiv](https://arxiv.org/abs/2302.13971) [GitHub](https://github.com/facebookresearch/llama) |
| 29 Jan 2023 | Text+Chem T5 | Unifying Molecular and Textual Representations via Multi-task Language Modelling | [arXiv](https://arxiv.org/abs/2301.12586) |
| 19 April 2023 | Baichuan2-13B | Baichuan 2: Open Large-scale Language Models | [arXiv](https://arxiv.org/abs/2309.10305) [GitHub](https://github.com/baichuan-inc/Baichuan2) |
| 16 Nov 2022 | Galactica | Galactica: A Large Language Model for Science | [arXiv](https://arxiv.org/abs/2211.09085) [GitHub](https://github.com/paperswithcode/galai) |
| 5 Oct 2022 | PaLM | PaLM: Scaling Language Modeling with Pathways | [arXiv](https://arxiv.org/abs/2204.02311) |
| 20 Oct 2022 | Flan-T5 (Flan-T5-base, Flan-T5-xl) | Scaling Instruction-Finetuned Language Models | [arXiv](https://arxiv.org/abs/2210.11416) [GitHub](https://github.com/google-research/t5x) |
| Nov 30 2022 | GPT-3.5 | GPT-3.5 Technical Report | [Blog](https://openai.com/blog/chatgpt) |
| 5 Aug 2022 | BlenderBot | BlenderBot 3: A Deployed Conversational Agent That Continually Learns to Responsibly Engage | [arXiv](https://arxiv.org/abs/2208.03188) |
| Sep 2022 | GIZA++ | Embedding-Enhanced GIZA++: Improving Word Alignment Using Embeddings | [Paper](https://aclanthology.org/2022.amta-research.20.pdf) |
| 25 Mar 2022 | CodeGen | CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis | [arXiv](https://arxiv.org/abs/2203.13474) [GitHub](https://github.com/salesforce/CodeGen) |
| 12 April 2022 | INCODER-1B,6B | InCoder: A Generative Model for Code Infilling and Synthesis | [arXiv](https://arxiv.org/abs/2204.05999) [GitHub](https://github.com/dpfried/incoder) |
| 10 April 2022 | RewardRepair | Neural Program Repair with Execution-based Backpropagation | [arXiv](https://arxiv.org/abs/2105.04123) |
| 2 May 2022 | OPT (OPT, OPT-13B, OPT-6.7B, OPT-1.3B) | OPT: Open Pre-trained Transformer Language Models | [arXiv](https://arxiv.org/abs/2205.01068) [GitHub](https://github.com/facebookresearch/metaseq) |
| 29 Apr 2021 | EFL | Entailment as Few-Shot Learner | [arXiv](https://arxiv.org/abs/2104.14690) |
| 18 Nov 2021 | DeBERTaV3-base | DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing | [arXiv](https://arxiv.org/abs/2111.09543) [GitHub](https://github.com/microsoft/DeBERTa) |
| 2 Sep 2021 | CodeT5 | CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation | [arXiv](https://arxiv.org/abs/2109.00859) [GitHub](https://github.com/salesforce/CodeT5) |
| Aug 2021 | GPT-Neo | GPT-Neo: Implementation of Model Parallel GPT2-like Models | [GitHub](https://github.com/EleutherAI/gpt-neo) |
| 7 May 2021 | CURE | Code-Aware Neural Machine Translation for Automatic Program Repair | [Paper](https://ieeexplore.ieee.org/abstract/document/9401997) |
| 28 May 2020 | GPT-3 | Language Models are Few-Shot Learners | [arXiv](https://arxiv.org/abs/2005.14165) |
| 5 June 2020 | DeBERTaLARGE,DeBERTa | DeBERTa: Decoding-enhanced BERT with Disentangled Attention | [arXiv](https://arxiv.org/abs/2006.03654) [GitHub](https://github.com/microsoft/DeBERTa) |
| 18 Apr 2020 | SimAlign | SimAlign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings | [arXiv](https://arxiv.org/abs/2004.08728) [GitHub](https://github.com/cisnlp/simalign) |
| 10 Apr 2020 | Longformer | Longformer: The Long-Document Transformer | [arXiv](https://arxiv.org/abs/2004.05150) [GitHub](https://github.com/allenai/longformer) |
| 6 Apr 2020 | MobileBERT | MobileBERT: A Compact Task-Agnostic BERT for Resource-Limited Devices | [arXiv](https://arxiv.org/abs/2004.02984) |
| 27 April 2020 | ColBERT | ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT | [arXiv](https://arxiv.org/abs/2004.12832) [GitHub](https://github.com/stanford-futuredata/ColBERT) |
| 23 Mar 2020 | ELECTRA | ELECTRA: Pre-training Text Encodors as Discriminators Rather Than Generators | [arXiv](https://arxiv.org/abs/2003.10555) [GitHub](https://github.com/google-research/electra) |
| 4 Mar 2020 | jiant | jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models | [arXiv](https://arxiv.org/abs/2003.02249) [GitHub](https://github.com/nyu-mll/jiant) |
| 14 Feb 2020 | TwinBERT | TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Efficient Retrieval | [arXiv](https://arxiv.org/abs/2002.06275) |
| 29 Oct 2019 | BART-Large | BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | [arXiv](https://arxiv.org/abs/1910.13461) [GitHub](https://github.com/facebookresearch/fairseq/tree/main/examples/bart) |
| 23 Oct 2019 | T5 (T5-BASE, T5-SMALL, T5-Large) | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | [arXiv](https://arxiv.org/abs/1910.10683) [GitHub](https://github.com/google-research/text-to-text-transfer-transformer) |
| 8 Nov 2019 | mBART | How Language-Neutral is Multilingual BERT? | [arXiv](https://arxiv.org/abs/1911.03310) [GitHub](https://github.com/facebookresearch/fairseq/tree/main/examples/mbart) |
| 5 Nov 2019 | XLM-R (xlm-roberta-base) | Unsupervised Cross-lingual Representation Learning at Scale | [arXiv](https://arxiv.org/abs/1911.02116) [GitHub](https://github.com/pytorch/fairseq/tree/main/examples/xlmr) |
| 1 Nov 2019 | DialoGPT | DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation | [arXiv](https://arxiv.org/abs/1911.00536) [GitHub](https://github.com/microsoft/DialoGPT) |
| 2 Oct 2019 | DistilBERT | DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter | [arXiv](https://arxiv.org/abs/1910.01108) [GitHub](https://github.com/huggingface/transformers/tree/main/src/transformers/models/distilbert) |
| 26 Sep 2019 | ALBERT | ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | [arXiv](https://arxiv.org/abs/1909.11942) [GitHub](https://github.com/google-research/albert) |
| 26 July 2019 | RoBERTa (RoBERTa-Large, RoBERTa-Base) | RoBERTa: A Robustly Optimized BERT Pretraining Approach | [arXiv](https://arxiv.org/abs/1907.11692) [GitHub](https://github.com/pytorch/fairseq/tree/main/examples/roberta) |
| 19 June 2019 | XLNet | XLNet: Generalized Autoregressive Pretraining for Language Understanding | [arXiv](https://arxiv.org/abs/1906.08237) [GitHub](https://github.com/zihangdai/xlnet) |
| Feb 2019 | GPT2 (GPT2, GPT2-M, GPT2-L) | Language Models are Unsupervised Multitask Learners | [Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [GitHub](https://github.com/openai/gpt-2) |
| 11 Oct 2018 | BERT-Base,Large | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | [arXiv](https://arxiv.org/abs/1810.04805) [GitHub](https://github.com/google-research/bert) |
| 12 June 2017 | Transformer | Attention Is All You Need | [arXiv](https://arxiv.org/abs/1706.03762) [GitHub](https://github.com/tensorflow/tensor2tensor) |
| July 2002 | BLEU | BLEU: A Method for Automatic Evaluation of Machine Translation | [Paper](https://aclanthology.org/P02-1040.pdf) |


# PEFT Methods in Vision Models


## Models and Papers

| Year | Model | Paper Title | Links |
|------|-------|-------------|-------|
| 7 Mar 2025 | FaceT-B, PLFace | Efficient Fine-tuning Strategies for Enhancing Face Recognition Performance in Challenging Scenarios |[Paper](https://ieeexplore.ieee.org/abstract/document/10887682) |
| 5 Apr 2023 | SAM | Segment Anything | [arXiv](https://arxiv.org/abs/2304.02643) [GitHub](https://github.com/facebookresearch/segment-anything) |
| 24 Mar 2023 | Point-PEFT | Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models | [Paper](https://ojs.aaai.org/index.php/AAAI/article/view/28323) |
| 16 Mar 2023 | ESM2 | Evolutionary-scale prediction of atomic protein structure with a language model | [Paper](https://www.science.org/doi/10.1126/science.ade2574) [GitHub](https://github.com/facebookresearch/esm) |
| 20 Dec 2022 | DiT | Scalable Diffusion Models with Transformers | [arXiv](https://arxiv.org/abs/2212.09748) [GitHub](https://github.com/facebookresearch/DiT) |
| 21 Feb 2022 | PointM2AE | Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training | [Paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/ad1d7a4df30a9c0c46b387815a774a84-Abstract-Conference.html) |
| 23 Mar 2022 | VideoMAE | VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training | [arXiv](https://arxiv.org/abs/2203.12602) [GitHub](https://github.com/MCG-NJU/VideoMAE) |
| 16 Mar 2022 | CodeReviewer | Automating Code Review Activities by Large Language Models | [arXiv](https://arxiv.org/abs/2203.09095) |
| 13 Mar 2022 | Point-MAE | Point-MAE: Masked Autoencoders for 3D Point Cloud Self-supervised Learning | [arXiv](https://arxiv.org/abs/2203.06604) [GitHub](https://github.com/Pang-Yatian/Point-MAE) |
| 11 Jan 2022 | ConvNeXt | A ConvNet for the 2020s | [arXiv](https://arxiv.org/abs/2201.03545) [GitHub](https://github.com/facebookresearch/ConvNeXt) |
| 29 Nov 2021 | Point-BERT | Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling | [arXiv](https://arxiv.org/abs/2111.14819) [GitHub](https://github.com/lulutang0608/Point-BERT) |
| 11 Nov 2021 | MAE | Masked Autoencoders Are Scalable Vision Learners | [arXiv](https://arxiv.org/abs/2111.06377) [GitHub](https://github.com/facebookresearch/mae) |
| 24 Jun 2021 | Swin Video Transformer | Video Swin Transformer | [arXiv](https://arxiv.org/abs/2106.13230) [GitHub](https://github.com/SwinTransformer/Video-Swin-Transformer) |
| 31 May 2021 | Segformer | SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers | [arXiv](https://arxiv.org/abs/2105.15203) [GitHub](https://github.com/NVlabs/SegFormer) |
| 18 Jul 2021 | AS-MLP | AS-MLP: An Axial Shifted MLP Architecture for Vision | [arXiv](https://arxiv.org/abs/2107.08391) |
| 25 Mar 2021 | Swin Transformer (Swin-L, Swin-B) | Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [arXiv](https://arxiv.org/abs/2103.14030) [GitHub](https://github.com/microsoft/Swin-Transformer) |
| 26 Feb 2021 | CLIP | Learning Transferable Visual Models From Natural Language Supervision | [arXiv](https://arxiv.org/abs/2103.00020) [GitHub](https://github.com/openai/CLIP) |
| 24 Feb 2021 | PVT | Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions | [arXiv](https://arxiv.org/abs/2102.12122) [GitHub](https://github.com/whai362/PVT) |
| 11 Feb 2021 | NFNets | High-Performance Large-Scale Image Recognition Without Normalization | [arXiv](https://arxiv.org/abs/2102.06171) [GitHub](https://github.com/deepmind/deepmind-research/tree/master/nfnets) |
| 31 Dec 2020 | SETR | Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers | [arXiv](https://arxiv.org/abs/2012.15840) [GitHub](https://github.com/fudan-zvg/SETR) |
| 23 Dec 2020 | DeiT | Training data-efficient image transformers & distillation through attention | [arXiv](https://arxiv.org/abs/2012.12877) [GitHub](https://github.com/facebookresearch/deit) |
| 22 Oct 2020 | ViT (ViT-B/16, ViT-S) | An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | [arXiv](https://arxiv.org/abs/2010.11929) [GitHub](https://github.com/google-research/vision_transformer) |
| 7 Mar 2020 | RegNetX | Designing Network Design Spaces | [arXiv](https://arxiv.org/abs/2003.13678) [GitHub](https://github.com/facebookresearch/pycls) |
| 13 Feb 2020 | SimCLR | A Simple Framework for Contrastive Learning of Visual Representations | [arXiv](https://arxiv.org/abs/2002.05709) [GitHub](https://github.com/google-research/simclr) |
| 30 Jun 2018 | SELDnet | Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks | [arXiv](https://arxiv.org/abs/1807.00129) |
| 17 Mar 2017 | ProtoNet | Prototypical Networks for Few-shot Learning | [arXiv](https://arxiv.org/abs/1703.05175) [GitHub](https://github.com/jakesnell/prototypical-networks) |
| 23 May 2016 | WideResNet | Wide Residual Networks | [arXiv](https://arxiv.org/abs/1605.07146) [GitHub](https://github.com/szagoruyko/wide-residual-networks) |
| 10 Dec 2015 | ResNet (ResNet-50, ResNet-101) | Deep Residual Learning for Image Recognition | [arXiv](https://arxiv.org/abs/1512.03385) [GitHub](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py) |
| 4 Sep 2014 | VGG (VGG16, VGG19) | Very Deep Convolutional Networks for Large-Scale Image Recognition | [arXiv](https://arxiv.org/abs/1409.1556) [GitHub](https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py) |





# üöÄ PEFT Methods in NLP Tasks Datasets

| Dataset        | Task Type                                          | Dataset Size         | Dataset Link |
|----------------|----------------------------------------------------|-----------------------|--------------|
| 20 Newsgroups  | Text Classification                                | 20K                  |[Link](https://archive.ics.uci.edu/dataset/113/twenty+newsgroups)
| CoNLL03        | Named Entity Recognition                           | 2302 articles        |[Link](http://arxiv.org/abs/cs/0306050)|
| CoNLL04        | Relation Extraction Tasks                          | 1,437 sentences      |[Link](https://aclanthology.org/W04-2401/)              |
| SST-2          | Sentiment Classification                           | 215,154              |[Link](https://aclanthology.org/D13-1170/)             |
| MRPC           | Paraphrase Detection                               | 5,800                |[Link](https://aclanthology.org/I05-5002/)              |
| ACE2005        | Information Extraction (IE)                        | 599 documents        |[Link](https://github.com/ualbertalib/metadata/blob/master/metadata-wrangling/ldc_metadata/ual_ldc_marc.mrc)              |
| WMT            | Machine Translation                                | N/A                  |[Link](https://aclanthology.org/2022.wmt-1.1.pdf)              |
| MPQA           | Opinion Mining                                     | 535 (articles)       |[Link](https://www.cs.cornell.edu/home/cardie/papers/lre05withappendix.pdf)              |
| Common Crawl   | Web Scraping, NLP Pretraining                      | 386 TiB              |[Link](https://commoncrawl.org/)              |
| i2b2 2010 RE   | Relation Extraction in Clinical Text               | 877 documents        |[Link](https://pmc.ncbi.nlm.nih.gov/articles/PMC3168320/)              |
| Semeval-2010   | Semantic Role Labeling, WSD, Relation Extraction   | 284                  |[Link](https://aclanthology.org/S10-1004/)              |
| COPA           | Commonsense Causal Reasoning                       | 1000 questions       |[Link](https://aclanthology.org/S12-1052/)              |
| IMDb           | Sentiment Classification                           | 50k reviews          |[Link](https://aclanthology.org/P11-1015/)              |
| SBU            | Image Captioning                                   | 1 million images     |[Link](https://ieeexplore.ieee.org/document/8953407)              |
| WebQ           | Question Answering                                 | 6,642                |[Link](https://fcon_1000.projects.nitrc.org/indi/abide/abide_I.html)              |
| OntoNotes      | NER, Coreference Resolution                        | 2.9453M              |[Link](https://aclanthology.org/D13-1160/)              |
| DUT            | Salient Object Detection                           | 5,168 images         |[Link](https://ieeexplore.ieee.org/document/6619251)              |
| AG News        | Text Classification                                | 1M                   |[Link](https://arxiv.org/abs/1509.01626)              |
| Yelp           | Sentiment Classification                           | 700,000              |[Link](https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset)             |
| VQA v2.0       | Visual Question Answering                          | 265,016              |[Link](https://arxiv.org/pdf/1509.01626)              |
| TriviaQA       | Question Answering                                 | 650K                 |[Link](https://arxiv.org/html/2406.11811v1)              |
| SQuAD          | Question Answering                                 | 100K                 |[Link](https://aclanthology.org/D16-1264/)              |
| BoolQ          | Boolean Question Answering                         | 15,942               |[Link](https://github.com/google-research-datasets/boolean-questions)             |
| YELP-Polarity   | Sentiment Analysis (Binary Classification)       | 560,000              |[Link](https://paperswithcode.com/dataset/yelp-review-polarity)              |
| CNN/DailyMail   | Abstractive Summarization                         | 313k                 |[Link](https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail)              |
| Yelp            | Sentiment Classification                         | 700,000              |[Link](https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset)              |
| WMT 16 en-ro    | Machine Translation (English to Romanian and vice versa) | 2 million sentences  |[Link](https://arxiv.org/abs/1606.02891)              |
| MAWPS           | Math Word Problem Solving                         | 2,373 problems        |[Link](https://aclanthology.org/N16-1136/)              |
| MIMIC-III MP    | Medical Predictive Modeling                       | 60,000 ICU stays      |[Link](https://pmc.ncbi.nlm.nih.gov/articles/PMC4878278/)              |
| WMT 16 en-ro    | Machine Translation (English to Romanian and vice versa) | 2 million sentences  |[Link](https://arxiv.org/abs/1606.02891)              |
| E2E             | Data to Text Generation                           | N/A                  |[Link](https://paperswithcode.com/sota/data-to-text-generation-on-e2e)              |
| MNLI            | Natural Language Inference                        | 433k                 |[Link](https://paperswithcode.com/dataset/multinli)              |
| WebNLG          | Natural Language Generation                       | 13,211 triples        |[Link](https://synalp.gitlabpages.inria.fr/webnlg-challenge/files/2020.webnlg-papers.7.pdf)              |
| STS-B           | Semantic Textual Similarity                       | 8,628 pairs           |[Link](https://aclanthology.org/S17-2001/)              |
| HS              | Hate Speech Detection                             | 25,000 tweets         |[Link](https://github.com/t-davidson/hate-speech-and-offensive-language)              |
| WikiSQL         | Text-to-SQL Generation                            | 80,654                |[Link](https://paperswithcode.com/dataset/wikisql)              |
| ARC             | Multiple Choice Question Answering               | 7,787                |[Link](https://arxiv.org/abs/1803.05457)              |
| GLUE            | Natural Language Understanding                    | N/A                  |[Link](https://arxiv.org/abs/1804.07461)              |
| OBQA            | Commonsense Question Answering                    | 5,957 questions       |[Link](https://aclanthology.org/D18-1260/)              |
| ARC-Easy        | Multiple-choice Question Answering                | 5,197                |[Link](https://arxiv.org/abs/1803.05457)              |
| ARC-Challenge   | Multiple-choice Question Answering                | 2,590                |[Link](https://arxiv.org/abs/1803.05457)              |
| MultiRC                | Multi-Sentence Reading Comprehension                | 10,000 questions          |[Link](https://aclanthology.org/N18-1023/)              |
| XSum                   | Abstractive Summarization                           | 226,711                   |[Link](https://www.creativefabrica.com/product/narayana/)              |
| CoLA                   | Sentence Acceptability                              | 10,657 sentences          |[Link](https://github.com/nyu-mll/CoLA/tree/master)              |
| SciTail dataset        | Textual Entailment                                  | 27,026 examples           |[Link](https://github.com/tensorflow/datasets/blob/master/docs/catalog/sci_tail.md)              |
| SQuAD 2.0              | Question Answering                                  | 150,000+ question-answer pairs |[Link](https://arxiv.org/abs/1606.05250)         |
| PIQA                   | Multiple Choice Question Answering                  | 19K                       |[Link](https://arxiv.org/abs/1911.11641)              |
| Winogrande             | Commonsense Reasoning                               | 44K                       |[Link](https://arxiv.org/abs/1907.10641)              |
| OSCAR                  | Pre-training Language Models                        | 50K                       |[Link](https://oscar-project.org/publication/2019/clmc7/asynchronous/)              |
| SuperGLUE              | Natural Language Understanding (NLU)                | N/A                       |[Link](https://arxiv.org/abs/1804.07461)              |
| codeSearchNet          | --                                                  | 2 million                 |[Link](https://arxiv.org/abs/1909.09436)              |
| AmazonQA/ProductQA     | Question Answering (QA)                             | 923K questions, 3.6M answers |[Link](https://arxiv.org/abs/1908.04364)           |
| commonsenseQA (CSQA)   | Question Answering                                  | 12,247 questions          |[Link](https://aclanthology.org/N19-1421/)              |
| SAMSum                 | Dialogue Summarization                              | 16,369                    |[Link](https://aclanthology.org/D19-5409/)              |
| Word-in-Context (WiC)  | Evaluate Context-Sensitive Meaning                  | 7,466                     |[Link](https://aclanthology.org/N19-1128/)              |
| Hyperpartisan          | Binary Classification (Hyperpartisan vs. Non-Hyperpartisan) | 754,000 articles     |[Link](https://aclanthology.org/S19-2145/)              |
| PAWS                   | Paraphrase Identification                           | 108,463                   |[Link](https://aclanthology.org/N19-1131/)              |
| WoW                    | Knowledge-driven Dialogue Generation                | N/A                       |[Link](https://openreview.net/forum?id=r1l73iRqKm)              |
| CosmosQA               | Commonsense Reading Comprehension                   | 35.6K questions           |[Link](https://aclanthology.org/D19-1243/)              |
| HellaSwag              | Commonsense Reasoning / Sentence Completion         | N/A                       |[Link](https://aclanthology.org/P19-1472/)              |
| MMLU                         | Multiple-Choice QA                            | 15,000+ questions across 57 subjects |[Link](https://arxiv.org/abs/2009.03300)              |
| The Pile                     | Language Modeling                             | 825GB                             |[Link](https://arxiv.org/abs/2101.00027)              |
| WikiTableText                | Data-to-Text / QA                             | 100M+ tokens                      |[Link](https://arxiv.org/abs/1609.07843)              |
| DART                         | Data-to-Text                                  | 82,191 examples                   |[Link](https://aclanthology.org/2021.naacl-main.37/)              |
| C4                           | Language Modeling / Pre-training              | 806.87 GiB                        |[Link](https://arxiv.org/abs/1910.10683)              |
| LibriLight                   | Self-supervised Speech                        | 60,000 hours                      |[Link](https://awwa.onlinelibrary.wiley.com/doi/full/10.1002/aws2.1179)              |
| ANLI                         | Natural Language Inference                    | 162,865 examples                  |[Link](https://csrc.nist.gov/pubs/ai/100/2/e2023/final)              |
| COMMONGEN                    | Text Generation                               | 79K descriptions over 35K concept sets |[Link](https://aclanthology.org/2020.findings-emnlp.165/)           |
| GSM8K (Grade School Math 8K) | Mathematical Problem Solving                  | 8,500                             |[Link](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/gsm8k/README.md)              |
| MATH                         | Mathematical Problem Solving                  | 12,500                            |[Link](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/gsm8k/README.md)              |
| TruthfulQA                   | Question Answering, Text Generation           | 817                               |[Link](https://arxiv.org/abs/2109.07958)              |
| Therapeutics Data Commons benchmark | AI-driven drug discovery            | 66 AI-ready datasets              |[Link](https://pubmed.ncbi.nlm.nih.gov/34305919/)              |
| SVAMP                        | Math Word Problem Solving                     | 1,000 problems                    |[Link](https://github.com/arkilpatel/SVAMP)              |
| CUB-200-2011   | Fine-grained Image Classification                  | 11,788 images              |[Link](https://authors.library.caltech.edu/records/cvm3y-5hh21)              |
| EL EVATER      | Multimodal Entity Linking, Visual Attribute Recognition | 10,000 instances (text-image pairs) |[Link](https://w3.jamstec.go.jp/apl/j/members/sasaki/ofes_publication.html)     |
| TB-1k          | Medical Image Classification                       | 1,000K                     |[Link](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)              |
| ro-en          | --                                                 | 614,318 sentences          |N/A              |
| de-en          | --                                                 | 4,554,053 sentences        |N/A              |
| Ha‚ÄìEn          | --                                                 | 50,000 sentences           |N/A              |
| (De-En)        | --                                                 | N/A                        |N/A              |
| Fr‚ÄìDe          | --                                                 | N/A                        |N/A              |
| Fr‚ÄìEs          | --                                                 | N/A                        |N/A              |
| Flan v2                  | Instruction tuning for NLP tasks                   | 1,800+                              |[Link](https://jasonwei20.github.io/files/FLAN%20talk%20external.pdf)              |
| BBH                      | Question-Answering                                 | 6,511                               |[Link](https://arxiv.org/abs/2210.09261)              |
| Pix2Struc                | Visual Language Understanding                      | N/A                                 |[Link](https://arxiv.org/abs/2210.03347)              |
| M2D2                     | Text                                               | 8.5B tokens across 145 domains      |[Link](https://aclanthology.org/2022.emnlp-main.63/)              |
| Alpaca                   | Instruction Following                              | 52K                                 |[Link](https://crfm.stanford.edu/2023/03/13/alpaca.html)              |
| GPT-4 Alpaca             | Instruction-following fine-tuning                  | 52K                                 |[Link](https://github.com/tatsu-lab/stanford_alpaca)              |
| Dolly                    | Instruction-Following NLP                          | 15K prompts                         |[Link](https://aclanthology.org/N12-4001/)              |
| Orca                     | Instruction-Following, Reasoning                   | 1.6 million                         |[Link](https://arxiv.org/abs/2306.02707)              |
| GPT-4-Turbo              | --                                                 | 128k                                |N/A             |
| AI4Bharat Naamapadam     | NER on Indian languages                            | 400k sentences                      |[Link](https://aclanthology.org/2023.acl-long.582/)              |
| AmericasNLI              | Natural Language Inference                         | N/A                                 |N/A              |
| SIQA                     | Text                                               | 33,410                              |N/A              |
| TREC                     | Question Classification                            | 4,500                               |[Link](https://aclanthology.org/C02-1150/)              |
| ScienceQ                 | Science Question Answering                         | N/A                                 |N/A              |
| Wikitext2                | Language Modeling                                  | 2M                                  |[Link](https://arxiv.org/abs/1609.07843)              |
| Penn Treebank            | Language Modeling                                  | 1M                                  |N/A              |
| VQA v2.0                 | Visual Question Answering                          | 265,016                             |[Link](https://arxiv.org/abs/1612.00837)              |
| VisDA-C                  | Domain Adaptation for Image Classification         | 280k images                         |[Link](https://arxiv.org/abs/1710.06924)              |
| ImageNet-Sketch          | Image Classification                               | 50,889 images                       |[Link](https://papers.nips.cc/paper_files/paper/2019/hash/3eefceb8087e964f89c2d59e8a249915-Abstract.html)              |
| ImageNet-A               | Image Classification                               | 7,500 images                        |[Link](https://paperswithcode.com/dataset/imagenet-a)              |
| DocVQA                   | Visual Question Answering (VQA) on documents       | 50,000 questions, 12,767 images     |[Link](https://arxiv.org/abs/2007.00398)              |
| IWSLT                    | --                                                 | N/A                                 |N/A              |
| FGVC                     | Fine-grained object classification                 | 10,200 images of aircraft           |[Link](https://arxiv.org/abs/1306.5151)              |
| OPUS-100                         | Multilingual Machine Translation (text-to-text)                      | 55 million parallel sentences           |[Link](https://www.researchgate.net/publication/361063314_OneAligner_Zero-shot_Cross-lingual_Transfer_with_One_Rich-Resource_Language_Pair_for_Low-Resource_Sentence_Retrieval)              |
| MS COCO dataset                  | Object Detection, Instance Segmentation, Panoptic Segmentation, Image Captioning, Keypoint Detection | 118K images / 5K images / 41K images |[Link](https://arxiv.org/abs/1405.0312)              |
| COCO Stuff                       | Semantic Segmentation, Panoptic Segmentation, Scene Understanding    | 164,000 images                          |[Link](https://github.com/nightrome/cocostuff)              |
| PASCAL VOC                       | Object Detection, Semantic Segmentation, Image Classification, Action Recognition, Person Layout | VOC 2012: 11,530 images; Object instances: 27,450; Segmentation: 2,913 images with pixel masks |[Link](http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.html)              |
| MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge | Multi-organ Segmentation (13 abdominal organs)                     | 30 CT scans / 20 CT scans               |[Link](https://paperswithcode.com/dataset/miccai-2015-multi-atlas-abdomen-labeling)              |
| PASCAL Context                   | Object Detection/Segmentation, Semantic Segmentation, Scene Parsing  | 10,103 images with 65,937 labeled objects; Classes: 459 |[Link](https://openaccess.thecvf.com/content_cvpr_2014/html/Mottaghi_The_Role_of_2014_CVPR_paper.html)              |
| Pascal Context-59               | Semantic Segmentation, Scene Parsing                                 | 10,103 images with dense labels for 59 classes |[Link](https://openaccess.thecvf.com/content_cvpr_2014/html/Mottaghi_The_Role_of_2014_CVPR_paper.html)       |
| Pascal Context-459              | --                                                                    | N/A                                     |N/A              |
| ADE20K 150           | Semantic Segmentation, Scene Parsing, Instance Segmentation (optional) | 25,000 images                    |[Link](https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Scene_Parsing_Through_CVPR_2017_paper.html)               |
| ADE20K-847           | Semantic Segmentation, Scene Parsing, Instance Segmentation (optional) | 25,000 images                    |[Link](https://openaccess.thecvf.com/content_cvpr_2017/html/Zhou_Scene_Parsing_Through_CVPR_2017_paper.html)              |
| CIFAR-10             | Image Classification                                | 60,000 images                         |[Link](https://www.cs.toronto.edu/~kriz/cifar.html)              |
| CIFAR-100            | Image Classification                                | 60,000 images                         |[Link](https://www.cs.toronto.edu/~kriz/cifar.html)              |
| CIFAR-10-LT (Long Tail) | Image Classification                            | 60,000 images                         |[Link](https://www.cs.toronto.edu/~kriz/cifar.html)              |
| ImageNet1K           | Image Classification and Localization               | 1,431,167 images                      |[Link](https://arxiv.org/abs/1409.0575)              |
| ImageNet100          | Image Classification                                | 131,689 images                        |[Link](https://www.kaggle.com/datasets/ambityga/imagenet100)              |
| ORBIT                | Few-shot learning for teachable object recognition  | 2,687 videos                          |[Link](https://arxiv.org/abs/2104.03841)              |
| ScanObjectNN         | 3D Object Classification                            | 2,902 object instances                |[Link](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_EAST_An_Efficient_CVPR_2017_paper.pdf)              |
| ModelNet40           | 3D Shape Classification                             | 12,311 3D objects / 2,468 models      |[Link](https://www.kaggle.com/datasets/balraj98/modelnet40-princeton-3d-object-dataset)              |
| RESISC45             | Remote Sensing Scene Classification                 | 31,500 RGB images                     |[Link](https://paperswithcode.com/dataset/resisc45)              |
| CLEVR                | Visual Question Answering (VQA), Compositional Reasoning | 100,000 images / 1 million questions |[Link](https://cs.stanford.edu/people/jcjohns/clevr/)              |
| DepthTrack           | RGB-D Single Object Tracking                        | 150 sequences (100,000+ frames)       |[Link](https://opendatalab.com/OpenDataLab/CDTB)              |
| IconQA             | Visual Question Answering                        | N/A                               |N/A              |
| ImageNet-R         | Image Classification                             | 30,000 images                     |[Link](https://github.com/hendrycks/robustness/tree/master)              |
| Pix2Struc          | Visual Language Understanding                    | N/A                               |N/A              |
| M2D2 (2022)        | Multi-task NLP (NER, RE, QA, etc.)               | 8.5B tokens across 145 domains    |[Link](https://aclanthology.org/2022.emnlp-main.63/)              |
| Vizwiz             | Visual Question Answering                        | N/A                               |[Link](https://vizwiz.org/tasks-and-datasets/answer-grounding-for-vqa/)              |
| Flickr30k          | Image Captioning                                 | 31,783                            |[Link](https://huggingface.co/datasets/nlphuji/flickr30k)              |
| OKVQA              | Visual Question Answering                        | 14,055                            |[Link](https://paperswithcode.com/dataset/ok-vqa)              |
| OCR-VQA            | Optical Character Recognition                    | N/A                               |[Link](https://paperswithcode.com/dataset/ocr-vqa)              |
| LibriSpeech        | Automatic Speech Recognition                     | 1,000 hours                       |[Link](https://www.kaggle.com/datasets/pypiahmad/librispeech-asr-corpus)              |
| M2D2 (2022 again)  | Multi-task NLP (NER, RE, QA, etc.)               | 8.5B tokens across 145 domains    |[Link](https://github.com/machelreid/m2d2)              |
| ImageNet-Sketch    | Image Classification                             | 50,889 images                     |[Link](https://github.com/HaohanWang/ImageNet-Sketch)              |
| ImageNet-A         | Image Classification                             | 7,500 images                      |[Link](https://paperswithcode.com/dataset/imagenet-a)              |
| ImageNet-C         | Image Classification under Corruption            | 3,750,000 corrupted images        |[Link](https://paperswithcode.com/dataset/imagenet-c)              |
| VisDA-C            | Domain Adaptation for Image Classification       | 280K images                       |[Link](https://github.com/VisionLearningGroup/taskcv-2017-public)              |
| DomainNet-126      | Domain Adaptation for Image Classification       | 586,575 images                    |[Link](https://paperswithcode.com/dataset/domainnet)              |
| SBU                | Image Captioning                                 | 1 million images                  |[Link](https://opendatalab.com/OpenDataLab/SBU_Captions_Dataset)              |
| OxfordPets         | Fine-Grained Classification & Semantic Segmentation | 7,390 images                   |[Link](https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf)              |
| DUT                | Salient Object Detection                         | 5,168 images                      |[Link](https://www.kaggle.com/datasets/balraj98/duts-saliency-detection-dataset)              |
| DTD                   | Texture Classification                                | 5,640 images                          |[Link](https://arxiv.org/abs/1311.3618)              |
| Food101               | Food Image Classification                             | 101,000 images                        |[Link](https://opendatalab.com/OpenDataLab/Food-101)              |
| Kinetics-400          | Action Recognition, Video Classification              | 650k video clips                      |[Link](https://paperswithcode.com/dataset/kinetics-400-1)              |
| Something-Something-v2| Temporal Action Recognition                           | 220,847 videos                        |[Link](https://arxiv.org/abs/1706.04261)              |
| EuroSAT               | Land Use/Land Cover Classification                    | 27,000 images                         |[Link](https://github.com/phelber/EuroSAT)              |
| iNaturalist 2018      | Fine-Grained Species Classification                   | 611,753 images                        |[Link](https://forum.inaturalist.org/t/published-papers-that-use-inaturalist-data-wiki-4-2024/47837)              |
| IMD20                 | --                                                    | 35,000 images                         |[Link](https://staff.utia.cas.cz/novozada/db/)              |
| COD10K                | Camouflaged Object Detection                          | 10,000 images                         |[Link](https://paperswithcode.com/dataset/cod10k)              |
| CAMO                  | Camouflaged Object Segmentation                       | 1,250 images                          |[Link](https://opendatalab.com/OpenDataLab/CAMO)              |
| ISTD                  | Shadow Detection & Removal                            | N/A                                   |[Link](https://opendatalab.com/OpenDataLab/ISTD)              |
| UCF101                | Action Recognition, Video Classification              | 13,320 videos (7.2 GB)                |[Link](https://arxiv.org/abs/1212.0402)              |
| SUN397                | Scene Classification                                  | 108,753 images                        |[Link](https://www.mindspore.cn/docs/en/r2.3.0/api_python/dataset/mindspore.dataset.SUN397Dataset.html)              |
| ABIDE                 | Neuroimaging Classification (ASD vs. Controls)        | 1,112 subjects                        |[Link](https://fcon_1000.projects.nitrc.org/indi/abide/abide_I.html)              |
| Places-LT             | Long-Tailed Scene Classification                      | 62,500 images                         |[Link](https://paperswithcode.com/dataset/places-lt)              |
| CASIA                 | --                                                    | N/A                                   |[Link](https://www.kaggle.com/datasets/sophatvathana/casia-dataset)              |
| IMD20                 | --                                                    | N/A                                   |[Link](https://staff.utia.cas.cz/novozada/db/)              |
| CUHK                 | --                                                    | N/A                                    |[Link](https://www.kaggle.com/datasets/arbazkhan971/cuhk-face-sketch-database-cufs)              |
| CHAMELEON (Confused)  | --                                                    | N/A                                   |[Link](https://paperswithcode.com/dataset/chameleon-48-32-20-fixed-splits)              |
| Kinetics-700          | --                                                    | N/A                                   |[Link](https://paperswithcode.com/dataset/kinetics-700)              |
| DF-20M mini           | --                                                    | N/A                                   |[Link](https://paperswithcode.com/dataset/df20-mini)              |
| RIGA+ (Not found)     | --                                                    | 55                                    |[Link](https://zenodo.org/records/6325549)              |
| SCGM (Not found)      | --                                                    | N/A                                   |[Link](https://github.com/nijingchao/SCGM)              |
| VQA v2.0              | Visual Question Answering                             | 265,016                               |[Link](https://paperswithcode.com/dataset/visual-question-answering-v2-0)              |


## üßí <span id="head1"> *Contribution* </span>

<a href="https://github.com/Nusrat-Prottasha/PEFT-A2Z/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=Nusrat-Prottasha/PEFT-A2Z" />
</a>

