<h1 align="center">‚ú® Awesome PEFT</em> ‚ú®</h1>
<h2 align="center"> PEFT State of the Art Parameter Efficient Fine Tuning Techniques</em> </h2>

<p align="center">
  <a href="https://github.com/hee9joon/Awesome-Diffusion-Models">
    <img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" alt="Awesome Badge">
  </a>
</p>
<p align="center">

  <img src="https://img.shields.io/github/stars/Nusrat-Prottasha/PEFT-State-of-the-Art-Parameter-Efficient-Fine-Tuning-Techniques?color=orange&style=for-the-badge" alt="GitHub stars">
  <img src="https://img.shields.io/github/forks/Nusrat-Prottasha/PEFT-State-of-the-Art-Parameter-Efficient-Fine-Tuning-Techniques?color=blueviolet&style=for-the-badge" alt="GitHub forks">
  <img src="https://img.shields.io/github/last-commit/Nusrat-Prottasha/PEFT-State-of-the-Art-Parameter-Efficient-Fine-Tuning-Techniques?color=green&style=for-the-badge" alt="GitHub last commit">
  <img src="https://img.shields.io/github/issues/Nusrat-Prottasha/PEFT-State-of-the-Art-Parameter-Efficient-Fine-Tuning-Techniques?color=crimson&style=for-the-badge" alt="GitHub issues">
  
  <a href="https://GitHub.com/Nusrat-Prottasha/PEFT-State-of-the-Art-Parameter-Efficient-Fine-Tuning-Techniques/graphs/commit-activity">
    <img src="https://img.shields.io/badge/Maintained%3F-yes-brightgreen.svg?style=for-the-badge" alt="Maintenance">
  </a>

</p>

<p align="center">
    <img src="https://i.imgur.com/waxVImv.png" alt="Oryx Video-ChatGPT">
</p>

## üöÄ Serial Adapters

- Parameter-Efficient Transfer Learning for NLP [[Paper](https://arxiv.org/abs/1902.00751)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/google-research/adapter-bert)

- AdapterHub: A Framework for Adapting Transformers [[Paper](https://arxiv.org/abs/2007.07779)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/adapterhub-a-framework-for-adapting)

- MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer [[Paper](https://arxiv.org/abs/2005.00052)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/mad-x-an-adapter-based-framework-for-multi)

- Cross-Lingual Transfer with Target Language-Ready Task Adapters [[Paper](https://aclanthology.org/2023.findings-acl.13.pdf)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/parovicm/tlr-adapters?tab=readme-ov-file)

- BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning [[Paper](https://arxiv.org/pdf/1902.02671)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/AsaCooperStickland/Bert-n-Pals)



## üöÄ Parallel Adapters

- UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling [[Paper](https://arxiv.org/pdf/2302.06605)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/UniAdapter/UniAdapter)

- UniPT: Universal Parallel Tuning for Transfer Learning with Efficient Parameter and Memory [[Paper](https://arxiv.org/pdf/2308.14316)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/Paranioar/UniPT)

- AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition [[Paper](https://arxiv.org/pdf/2205.13535)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/ShoufaChen/AdaptFormer)

- BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning [[Paper](https://arxiv.org/pdf/1902.02671)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/AsaCooperStickland/Bert-n-Pals?tab=readme-ov-file)

- PEMT: Multi-Task Correlation Guided Mixture-of-Experts Enables Parameter-Efficient Transfer Learning [[Paper](https://arxiv.org/pdf/2402.15082)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/JachinLin2022/PEMT)

- Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference [[Paper](https://arxiv.org/pdf/2304.04947)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/conditional-adapters-parameter-efficient)


## üöÄ Hybrid Adapters

- AUTOPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning [[Paper](https://arxiv.org/pdf/2301.12132)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/cambridgeltl/autopeft)

- CROSS-MODAL ADAPTER: PARAMETER-EFFICIENT TRANSFER LEARNING APPROACH FOR VISION-LANGUAGE MODELS [[Paper](https://arxiv.org/pdf/2404.12588)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/cross-modal-adapter-parameter-efficient)

- EFFICIENT REMOTE SENSING WITH HARMONIZED TRANSFER LEARNING AND MODALITY ALIGNMENT [[Paper](https://arxiv.org/pdf/2404.18253)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/seekerhuang/HarMA?tab=readme-ov-file)

- MV-Adapter: Multimodal Video Transfer Learning for Video Text Retrieval [[Paper](https://arxiv.org/pdf/2301.07868)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/zhangbw17/MV-Adapter)

- Conv-Adapter: Exploring Parameter Efficient Transfer Learning for ConvNets [[Paper](https://arxiv.org/pdf/2208.07463)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/Hhhhhhao/Conv-Adapter)

- Conditional Adapters: Parameter-Efficient Transfer Learning with Fast Inference [[Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/19d7204af519eae9993f7f72377a0ec0-Paper-Conference.pdf)]
  ![NeurIPS](https://img.shields.io/badge/NeurIPS-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/conditional-adapters-parameter-efficient)


## üöÄ Single Task

- VISION TRANSFORMER ADAPTER FOR DENSE PREDICTIONS [[Paper](https://arxiv.org/pdf/2205.08534)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/czczup/ViT-Adapter)

- Simple, Scalable Adaptation for Neural Machine Translation [[Paper](https://aclanthology.org/D19-1165.pdf)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/simple-scalable-adaptation-for-neural-machine)

- K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters [[Paper](https://arxiv.org/pdf/2002.01808)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/microsoft/K-Adapter)


## üöÄ Multi Task

- K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters [[Paper](https://arxiv.org/pdf/2002.01808)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/microsoft/K-Adapter)

- AdapterFusion: Non-Destructive Task Composition for Transfer Learning [[Paper](https://arxiv.org/pdf/2005.00247)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/adapterfusion-non-destructive-task)

- OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy [[Paper](https://arxiv.org/pdf/2401.10559)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- Multi-Head Adapter Routing for Cross-Task Generalization [[Paper](https://arxiv.org/pdf/2211.03831)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/microsoft/mttl)

- Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks [[Paper](https://arxiv.org/pdf/2106.04489)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/rabeehk/hyperformer)

- When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications [[Paper](https://arxiv.org/pdf/2310.18339)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/liuqidong07/MOELoRA-peft)

- LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models [[Paper](https://aclanthology.org/2023.emnlp-main.319.pdf)]
  ![arXiv](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/AGI-Edgerunners/LLM-Adapters)

- AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models [[Paper](https://arxiv.org/pdf/2302.07027)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/adaptersoup-weight-averaging-to-improve)


## üöÄ Continuous Prompting 

- Prefix-Tuning: Optimizing Continuous Prompts for Generation [[Paper](https://arxiv.org/pdf/2101.00190)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/XiangLi1999/PrefixTuning)

- PEDRO: Parameter-Efficient Fine-tuning with Prompt DEpenDent Representation MOdification [[Paper](https://www.arxiv.org/pdf/2409.17834)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- DEPT: Decomposed Prompt Tuning for Parameter-Efficient Fine-Tuning [[Paper](https://arxiv.org/pdf/2309.05173)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/ZhengxiangShi/DePT)

- P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks [[Paper](https://arxiv.org/pdf/2110.07602)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/THUDM/P-tuning-v2)

- Q-PEFT: Query-dependent Parameter Efficient Fine-tuning for Text Reranking with Large Language Models [[Paper](https://arxiv.org/pdf/2404.04522)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- PTR: Prompt Tuning with Rules for Text Classification [[Paper](https://arxiv.org/pdf/2105.11259)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/thunlp/PTR)

- Prefix-Propagation: Parameter-Efficient Tuning for Long Sequences [[Paper](https://web3.arxiv.org/pdf/2305.12086)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/MonliH/prefix-propagation)

- Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts [[Paper](https://arxiv.org/pdf/2210.11292)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/xyltt/LPT)

- OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning [[Paper](https://arxiv.org/pdf/2402.04129)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jpmorganchase/ovor)


## üöÄ Discrete Prompt

- RLPROMPT: Optimizing Discrete Text Prompts with Reinforcement Learning [[Paper](https://arxiv.org/pdf/2205.12548)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/mingkaid/rl-prompt)

- SPARSEFIT: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations [[Paper](https://arxiv.org/pdf/2305.13235)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/AkihikoWatanabe/paper_notes/issues/1684)

- OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning [[Paper](https://arxiv.org/pdf/2402.04129)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jpmorganchase/ovor)

  


## üöÄ Domain Specific Adaption (Natural Language Understanding)

- Prompt Tuning Strikes Back: Customizing Foundation Models with Low-Rank Prompt Adaptation [[Paper](https://arxiv.org/pdf/2405.15282)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jabhinav/Prompt-Tuning-Strikes-Back-with-LOPA)

- InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding [[Paper](https://arxiv.org/pdf/2306.04933)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/infoprompt-information-theoretic-soft-prompt)

- PEFT-U: Parameter-Efficient Fine-Tuning for User Personalization [[Paper](https://arxiv.org/pdf/2407.18078)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/ChrisIsKing/Parameter-Efficient-Personalization)

- IDPG: An Instance-Dependent Prompt Generation Method [[Paper](https://arxiv.org/pdf/2204.04497)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/CSerxy/IDPG)

- APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models [[Paper](https://aclanthology.org/2023.emnlp-main.567.pdf)]
  ![EMNLP](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/wgcban/apt)

- SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts [[Paper](https://aclanthology.org/2023.emnlp-main.884.pdf)]
  ![EMNLP](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jyjohnchoi/SMoP)



## üöÄ Task Specific Adaption

- The Power of Scale for Parameter-Efficient Prompt Tuning [[Paper](https://arxiv.org/pdf/2104.08691)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/mkshing/Prompt-Tuning)

- SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer [[Paper](https://arxiv.org/pdf/2110.07904)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/spot-better-frozen-model-adaptation-through)

- APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference [[Paper](https://arxiv.org/html/2401.12200v2)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- XPROMPT: Exploring the Extreme of Prompt Tuning [[Paper](https://arxiv.org/pdf/2210.04457)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/BD-MF/XPrompt?tab=readme-ov-file)

- Parameter Efficient Multi-task Fine-tuning by Learning to Transfer Token-wise Prompts [[Paper](https://aclanthology.org/2023.findings-emnlp.584.pdf)]
  ![EMNLP](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/mlwu22/TPT)

- IDPG: An Instance-Dependent Prompt Generation Method [[Paper](https://arxiv.org/pdf/2204.04497)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/CSerxy/IDPG)

- APrompt: Attention Prompt Tuning for Efficient Adaptation of Pre-trained Language Models [[Paper](https://aclanthology.org/2023.emnlp-main.567.pdf)]
  ![EMNLP](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/wgcban/apt)

- SMoP: Towards Efficient and Effective Prompt Tuning with Sparse Mixture-of-Prompts [[Paper](https://aclanthology.org/2023.emnlp-main.884.pdf)]
  ![EMNLP](https://img.shields.io/badge/EMNLP-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jyjohnchoi/SMoP)




## üöÄ Scaling Adaption

- Propulsion: Steering LLM with Tiny Fine-Tuning [[Paper](https://arxiv.org/pdf/2409.10927)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/Kowsher/Propulsion)


## üöÄ Selective Tuning Based on Parameter Importance

- Parameter-Efficient Transfer Learning with Diff Pruning [[Paper](https://arxiv.org/pdf/2012.07463)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/dguo98/DiffPruning)

- Targeted Efficient Fine-tuning: Optimizing Parameter Updates with Data-Driven Sample Selection [[Paper](https://arxiv.org/pdf/2403.08484)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- Layer-wise Importance Matters: Less Memory for Better Performance in Parameter-efficient Fine-tuning of Large Language Models [[Paper](https://arxiv.org/pdf/2410.11772)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/Kaiseem/IST)

- AdaFish: Fast Low-Rank Parameter-Efficient Fine-Tuning by Using Second-Order Information [[Paper](https://arxiv.org/pdf/2403.13128v1)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/adafish-fast-low-rank-parameter-efficient)

- Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse Finetuning [[Paper](https://arxiv.org/pdf/2311.03748)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/psunlpgroup/FISH-DIP)



## üöÄ Unstructured Mask

- Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models [[Paper](https://arxiv.org/pdf/2305.16597)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/neural-architecture-search-for-parameter)

- Composable Sparse Fine-Tuning for Cross-Lingual Transfer [[Paper](https://arxiv.org/pdf/2110.07560)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/cambridgeltl/composable-sft)

- Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning [[Paper](https://arxiv.org/pdf/2109.05687)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/RunxinXu/ChildTuning)

- Parameter-Efficient Fine-Tuning without Introducing New Latency [[Paper](https://arxiv.org/pdf/2305.16742)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/parameter-efficient-fine-tuning-without)



## üöÄ Structured Mask


- Efficient Fine-Tuning of BERT Models on the Edge [[Paper](https://arxiv.org/pdf/2205.01541)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/efficient-fine-tuning-of-bert-models-on-the)

- Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation [[Paper](https://arxiv.org/pdf/2104.08771)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/MGheini/xattn-transfer-for-mt)

- X-PEFT: eXtremely Parameter-Efficient Fine-Tuning for Extreme Multi-Profile Scenarios [[Paper](https://arxiv.org/pdf/2401.16137)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- Structured Unrestricted-Rank Matrices for Parameter Efficient Fine-tuning [[Paper](https://ar5iv.labs.arxiv.org/html/2406.17740)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/arijitthegame/structured-matrices-PEFT)


## üöÄ Core Low Rank Decomposition


- LoRA: Low-Rank Adaptation of Large Language Models [[Paper](https://arxiv.org/pdf/2106.09685)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/microsoft/LoRA)

- Compacter: Efficient Low-Rank Hypercomplex Adapter Layers [[Paper](https://arxiv.org/pdf/2106.04647)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/rabeehk/compacter)

- Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning [[Paper](https://arxiv.org/pdf/2012.13255)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/intrinsic-dimensionality-explains-the)

- Parameter-Efficient Model Adaptation for Vision Transformers [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/25160)]
  ![AAAI](https://img.shields.io/badge/AAAI-4a4a4a?style=flat&labelColor=4a4a4a&color=4a4a4a)

- Parameter-Efficient Fine-Tuning without Introducing New Latency [[Paper](https://arxiv.org/pdf/2305.16742)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/parameter-efficient-fine-tuning-without)

- DoRA: Weight-Decomposed Low-Rank Adaptation [[Paper](https://arxiv.org/pdf/2402.09353)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/NVlabs/DoRA)

- LLMEmbed: Rethinking Lightweight LLM‚Äôs Genuine Function in Text Classification [[Paper](https://arxiv.org/pdf/2406.03725)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/ChunLiu-cs/LLMEmbed-ACL2024)



## üöÄ Adaptive and dynamic rank methods

- DyLoRA: Parameter-Efficient Tuning of Pretrained Models using Dynamic Search-Free Low Rank Adaptation [[Paper](https://arxiv.org/pdf/2210.07558)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/dylora-parameter-efficient-tuning-of-pre)

- AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning [[Paper](https://arxiv.org/pdf/2303.10512)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/QingruZhang/AdaLoRA)

- Sparse Low-Rank Adaptation of Pre-trained Language Models [[Paper](https://arxiv.org/pdf/2311.11696)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/TsinghuaC3I/SoRA)

- Increasing Model Capacity for Free: A Simple Strategy for Parameter-Efficient Fine-Tuning [[Paper](https://arxiv.org/pdf/2407.01320)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/LINs-lab/CapaBoost)

- AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning [[Paper](https://arxiv.org/pdf/2403.09113)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://anonymous.4open.science/r/AutoLoRA)



## üöÄ Enhanced LoRA variants for fine tuning efficiency

- Bayesian Low-Rank Adaptation for Large Language Models [[Paper](https://arxiv.org/html/2308.13111v5)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/MaximeRobeyns/bayesian_lora)

- LoRA Dropout as a Sparsity Regularizer for Overfitting Control [[Paper](https://arxiv.org/html/2404.09610v1)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/lora-dropout-as-a-sparsity-regularizer-for)

- PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization [[Paper](https://arxiv.org/pdf/2402.16141)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/periodiclora-breaking-the-low-rank-bottleneck)

- LoRA+: Efficient Low Rank Adaptation of Large Models [[Paper](https://arxiv.org/pdf/2402.12354)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/microsoft/LoRA)

- Mixture-of-Subspaces in Low-Rank Adaptation [[Paper](https://arxiv.org/pdf/2406.11909)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/wutaiqiang/MoSLoRA)

- Continual Learning with Low Rank Adaptation [[Paper](https://arxiv.org/pdf/2311.17601)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/continual-learning-with-low-rank-adaptation)

- Trans-LoRA: Towards Data-Free Transferable Parameter Efficient Finetuning [[Paper](https://arxiv.org/html/2405.17258v1)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/textit-trans-lora-towards-data-free)

- RoseLoRA: Row and Column-wise Sparse Low-Rank Adaptation [[Paper](https://arxiv.org/pdf/2406.10777)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/lliutianc/roselora)

- Low-Rank Few-Shot Adaptation of Vision-Language Models [[Paper](https://arxiv.org/html/2405.18541v2)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/MaxZanella/CLIP-LoRA)

- SVDQUANT: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models [[Paper](https://arxiv.org/pdf/2411.05007)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/mit-han-lab/nunchaku?tab=readme-ov-file)

- Variational Low-Rank Adaptation Using IVON [[Paper](https://arxiv.org/pdf/2411.04421)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/team-approx-bayes/ivon-lora)

- PMoL: Parameter Efficient MoE for Preference Mixing of LLM Alignment [[Paper](https://arxiv.org/pdf/2411.01245)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/pmol-parameter-efficient-moe-for-preference)

- Empower Vision Applications with LoRA LMM [[Paper](https://arxiv.org/pdf/2411.00915)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition [[Paper](https://arxiv.org/pdf/2307.13269)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/sail-sg/lorahub)

- MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications [[Paper](https://arxiv.org/pdf/2310.18339v1)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/liuqidong07/MOELoRA-peft)

- Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning [[Paper](https://arxiv.org/pdf/2309.05444)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/for-ai/parameter-efficient-moe)

- Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models [[Paper](https://arxiv.org/html/2403.03432v1)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/mixture-of-loras-an-efficient-multitask)

- MIXTURE OF LORA EXPERTS [[Paper](https://arxiv.org/pdf/2404.13628)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based Mixture of Experts [[Paper](https://arxiv.org/html/2404.15159v2)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/TUDB-Labs/MixLoRA)



## üöÄ Hybrid Approaches 


- Towards a Unified View of Parameter-Efficient Transfer Learning [[Paper](https://arxiv.org/pdf/2110.04366)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/jxhe/unify-parameter-efficient-tuning?tab=readme-ov-file)

- UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning [[Paper](https://arxiv.org/pdf/2110.07577)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/morningmoni/UniPELT)

- Parameter-Efficient Fine-Tuning Design Spaces [[Paper](https://arxiv.org/pdf/2301.01821)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/amazon-science/peft-design-spaces)

- Neural Prompt Search [[Paper](https://arxiv.org/pdf/2206.04673)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/ZhangYuanhan-AI/NOAH)

- AUTOPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning [[Paper](https://arxiv.org/pdf/2301.12132)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/cambridgeltl/autopeft)

- LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models [[Paper](https://arxiv.org/pdf/2304.01933)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/AGI-Edgerunners/LLM-Adapters)

- RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation [[Paper](https://arxiv.org/pdf/2401.04679)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/IST-DASLab/RoSA)

- Sparsity- and Hybridity-Inspired Visual Parameter-Efficient Fine-Tuning for Medical Diagnosis [[Paper](https://arxiv.org/pdf/2405.17877)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)

- HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks [[Paper](https://arxiv.org/pdf/2203.03878)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://paperswithcode.com/paper/hyperpelt-unified-parameter-efficient)

- Hydra: Multi-head Low-rank Adaptation for Parameter Efficient Fine-tuning [[Paper](https://arxiv.org/pdf/2309.06922)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/extremebird/Hydra)


## üöÄ MoE Based

- When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications [[Paper](https://arxiv.org/pdf/2310.18339)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/liuqidong07/MOELoRA-peft)

- MixLoRA: Enhancing Large Language Models Fine-Tuning with LoRA-based Mixture of Experts [[Paper](https://arxiv.org/pdf/2404.15159)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/TUDB-Labs/MixLoRA)

- Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning [[Paper](https://arxiv.org/pdf/2309.05444)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/for-ai/parameter-efficient-moe)

- Mixture-of-LoRAs: An Efficient Multitask Tuning for Large Language Models [[Paper](https://arxiv.org/pdf/2403.03432)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://gist.github.com/ruvnet/809d0312c1c599ba29721c93a20a741c)

- Mixture-of-Subspaces in Low-Rank Adaptation [[Paper](https://arxiv.org/pdf/2406.11909)]
  ![arXiv](https://img.shields.io/badge/arXiv-4a4a4a?style=flat&logo=arXiv&logoColor=white&labelColor=4a4a4a)
  [![Code](https://img.shields.io/badge/üë©‚Äçüíª%20Code-2962FF?style=flat&labelColor=2962FF&color=2962FF)](https://github.com/wutaiqiang/MoSLoRA)


# PEFT Methods in NLP Tasks

## Models and Papers

| Year | Model | Paper Title | Links |
|---------|-------|-------------|-------|
| 15 Oct 2024 | Bloomz | Investigating translation for Indic languages with BLOOMZ-3b through prompting and LoRA fine-tuning | [Paper](https://www.nature.com/articles/s41598-024-74617-9) |
| 31 July 2024 | LLaMA3-8B | The Llama 3 Herd of Models | [arXiv](https://arxiv.org/abs/2407.21783) [GitHub](https://github.com/meta-llama/llama3) |
| 18 Aug 2024 | Recorder | A Syntax-Guided Edit Decoder for Neural Program Repair | [Paper](https://dl.acm.org/doi/10.1145/3468264.3468544) |
| 18 June 2024 | ChatGLM-6B | ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools | [arXiv](https://arxiv.org/abs/2406.12793) [GitHub](https://github.com/THUDM/ChatGLM-6B) |
| 26 June 2024 | Tag-LLaMA | Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains | [arXiv](https://arxiv.org/abs/2402.05140) |
| 31 May 2024 | Mamba-2 | Mamba: Linear-Time Sequence Modeling with Selective State Spaces | [arXiv](https://arxiv.org/abs/2405.21060) [GitHub](https://github.com/state-spaces/mamba) |
| 15 May 2024 | LLaVA-1.5 | Improved Baselines with Visual Instruction Tuning | [arXiv](https://arxiv.org/abs/2310.03744) [GitHub](https://github.com/haotian-liu/LLaVA) |
| 16 April 2024 | GEMMA-2B, Gemma-7B | Gemma: Open Models Based on Gemini Research and Technology | [arXiv](https://arxiv.org/abs/2403.08295) [GitHub](https://github.com/google-gemma/gemma) |
| 16 Mar 2024 | Openchat8B | OpenChat: Advancing Open-source Language Models with Mixed-Quality Data | [arXiv](https://arxiv.org/abs/2309.11235) [GitHub](https://github.com/imoneoi/openchat) |
| 26 Feb 2024 | Airavata-7b | Airavata: Introducing Hindi Instruction-tuned LLM | [arXiv](https://arxiv.org/html/2401.15006v2) |
| 14 Feb 2024 | LlaSMol | LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset | [arXiv](https://arxiv.org/abs/2402.09391) [GitHub](https://github.com/OSU-NLP-Group/LLM4Chem) |
| 4 Jan 2024 | TinyLlama | TinyLlama: An Open-Source Small Language Model | [arXiv](https://arxiv.org/abs/2401.02385) [GitHub](https://github.com/jzhang38/TinyLlama) |
| 2024 | DeepSeek-Coder-Base-6.7B | DeepSeek Coder: When the Large Language Model Meets Programming | [arXiv](https://arxiv.org/abs/2401.14196) [GitHub](https://github.com/deepseek-ai/DeepSeek-Coder) |
| 2024 | GPT-J (6B) | GPT-J: 6B Parameter Open Source Transformer Model | [GitHub](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/GPT-J-6B) |
| 14 Dec 2024 | TigerBot-7B | TigerBot: A Multi-stage Open-Source LLM for Human and Agent Scenarios | [arXiv](https://arxiv.org/abs/2312.08688) [GitHub](https://github.com/TigerResearch/TigerBot) |
| 30 Nov 2023 | CCT5 | A Code-Change-Oriented Pre-trained Model | [Paper](https://dl.acm.org/doi/abs/10.1145/3611643.3616339) |
| 29 Nov 2023 | Falcon | The Falcon Series of Open Language Models | [arXiv](https://arxiv.org/pdf/2311.16867) |
| 21 Nov 2023 | ShareGPTv4(7B) | ShareGPT4V: Improving Large Multi-Modal Models with Better Captions | [arXiv](https://arxiv.org/abs/2311.12793) [GitHub](https://sharegpt4v.github.io/) |
| Dec 1 2023 | Mamba | Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality | [arXiv](https://arxiv.org/abs/2312.00752) [GitHub](https://github.com/state-spaces/mamba) |
| 10 Oct 2023 | Mistral-7B-Instruct | Mistral 7B | [arXiv](https://arxiv.org/abs/2310.06825) [GitHub](https://github.com/mistralai/mistral-inference) |
| 24 Aug 2023 | Qwen-VL-Chat(7B) | Qwen-VL: A Vision-Language Foundation Model for Universal Multimodal Understanding and Generation | [arXiv](https://arxiv.org/abs/2308.12966) [GitHub](https://github.com/QwenLM/Qwen-VL) |
| 24 Aug 2023 | CodeLlama-7B | Code Llama: Open Foundation Models for Code | [arXiv](https://arxiv.org/abs/2308.12950) [GitHub](https://github.com/facebookresearch/codellama) |
| 18 July 2023 | Llama2-7b,3b,13b,70b | Llama 2: Open Foundation and Fine-Tuned Chat Models | [arXiv](https://arxiv.org/abs/2307.09288) [GitHub](https://github.com/facebookresearch/llama) |
| 27 June 2023 | BLOOM-7B,1B | BLOOM: A 176B-Parameter Open-Access Multilingual Language Model | [arXiv](https://arxiv.org/abs/2211.05100) [GitHub](https://github.com/bigscience-workshop/multilingual-modeling) |
| 9 June 2023 | Vicuna-7b,13b | Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena | [arXiv](https://arxiv.org/abs/2306.05685) [GitHub](https://github.com/lm-sys/FastChat) |
| 15 Mar 2023 | GPT-4 | GPT-4 Technical Report | [arXiv](https://arxiv.org/abs/2303.08774) |
| 27 Feb 2023 | LLaMA30B | LLaMA: Open and Efficient Foundation Language Models | [arXiv](https://arxiv.org/abs/2302.13971) [GitHub](https://github.com/facebookresearch/llama) |
| 29 Jan 2023 | Text+Chem T5 | Unifying Molecular and Textual Representations via Multi-task Language Modelling | [arXiv](https://arxiv.org/abs/2301.12586) |
| 19 April 2023 | Baichuan2-13B | Baichuan 2: Open Large-scale Language Models | [arXiv](https://arxiv.org/abs/2309.10305) [GitHub](https://github.com/baichuan-inc/Baichuan2) |
| 16 Nov 2022 | Galactica | Galactica: A Large Language Model for Science | [arXiv](https://arxiv.org/abs/2211.09085) [GitHub](https://github.com/paperswithcode/galai) |
| 5 Oct 2022 | PaLM | PaLM: Scaling Language Modeling with Pathways | [arXiv](https://arxiv.org/abs/2204.02311) |
| 20 Oct 2022 | Flan-T5 (Flan-T5-base, Flan-T5-xl) | Scaling Instruction-Finetuned Language Models | [arXiv](https://arxiv.org/abs/2210.11416) [GitHub](https://github.com/google-research/t5x) |
| Nov 30 2022 | GPT-3.5 | GPT-3.5 Technical Report | [Blog](https://openai.com/blog/chatgpt) |
| 5 Aug 2022 | BlenderBot | BlenderBot 3: A Deployed Conversational Agent That Continually Learns to Responsibly Engage | [arXiv](https://arxiv.org/abs/2208.03188) |
| Sep 2022 | GIZA++ | Embedding-Enhanced GIZA++: Improving Word Alignment Using Embeddings | [Paper](https://aclanthology.org/2022.amta-research.20.pdf) |
| 25 Mar 2022 | CodeGen | CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis | [arXiv](https://arxiv.org/abs/2203.13474) [GitHub](https://github.com/salesforce/CodeGen) |
| 12 April 2022 | INCODER-1B,6B | InCoder: A Generative Model for Code Infilling and Synthesis | [arXiv](https://arxiv.org/abs/2204.05999) [GitHub](https://github.com/dpfried/incoder) |
| 10 April 2022 | RewardRepair | Neural Program Repair with Execution-based Backpropagation | [arXiv](https://arxiv.org/abs/2105.04123) |
| 2 May 2022 | OPT (OPT, OPT-13B, OPT-6.7B, OPT-1.3B) | OPT: Open Pre-trained Transformer Language Models | [arXiv](https://arxiv.org/abs/2205.01068) [GitHub](https://github.com/facebookresearch/metaseq) |
| 29 Apr 2021 | EFL | Entailment as Few-Shot Learner | [arXiv](https://arxiv.org/abs/2104.14690) |
| 18 Nov 2021 | DeBERTaV3-base | DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing | [arXiv](https://arxiv.org/abs/2111.09543) [GitHub](https://github.com/microsoft/DeBERTa) |
| 2 Sep 2021 | CodeT5 | CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation | [arXiv](https://arxiv.org/abs/2109.00859) [GitHub](https://github.com/salesforce/CodeT5) |
| Aug 2021 | GPT-Neo | GPT-Neo: Implementation of Model Parallel GPT2-like Models | [GitHub](https://github.com/EleutherAI/gpt-neo) |
| 7 May 2021 | CURE | Code-Aware Neural Machine Translation for Automatic Program Repair | [Paper](https://ieeexplore.ieee.org/abstract/document/9401997) |
| 28 May 2020 | GPT-3 | Language Models are Few-Shot Learners | [arXiv](https://arxiv.org/abs/2005.14165) |
| 5 June 2020 | DeBERTaLARGE,DeBERTa | DeBERTa: Decoding-enhanced BERT with Disentangled Attention | [arXiv](https://arxiv.org/abs/2006.03654) [GitHub](https://github.com/microsoft/DeBERTa) |
| 18 Apr 2020 | SimAlign | SimAlign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings | [arXiv](https://arxiv.org/abs/2004.08728) [GitHub](https://github.com/cisnlp/simalign) |
| 10 Apr 2020 | Longformer | Longformer: The Long-Document Transformer | [arXiv](https://arxiv.org/abs/2004.05150) [GitHub](https://github.com/allenai/longformer) |
| 6 Apr 2020 | MobileBERT | MobileBERT: A Compact Task-Agnostic BERT for Resource-Limited Devices | [arXiv](https://arxiv.org/abs/2004.02984) |
| 27 April 2020 | ColBERT | ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT | [arXiv](https://arxiv.org/abs/2004.12832) [GitHub](https://github.com/stanford-futuredata/ColBERT) |
| 23 Mar 2020 | ELECTRA | ELECTRA: Pre-training Text Encodors as Discriminators Rather Than Generators | [arXiv](https://arxiv.org/abs/2003.10555) [GitHub](https://github.com/google-research/electra) |
| 4 Mar 2020 | jiant | jiant: A Software Toolkit for Research on General-Purpose Text Understanding Models | [arXiv](https://arxiv.org/abs/2003.02249) [GitHub](https://github.com/nyu-mll/jiant) |
| 14 Feb 2020 | TwinBERT | TwinBERT: Distilling Knowledge to Twin-Structured BERT Models for Efficient Retrieval | [arXiv](https://arxiv.org/abs/2002.06275) |
| 29 Oct 2019 | BART-Large | BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension | [arXiv](https://arxiv.org/abs/1910.13461) [GitHub](https://github.com/facebookresearch/fairseq/tree/main/examples/bart) |
| 23 Oct 2019 | T5 (T5-BASE, T5-SMALL, T5-Large) | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer | [arXiv](https://arxiv.org/abs/1910.10683) [GitHub](https://github.com/google-research/text-to-text-transfer-transformer) |
| 8 Nov 2019 | mBART | How Language-Neutral is Multilingual BERT? | [arXiv](https://arxiv.org/abs/1911.03310) [GitHub](https://github.com/facebookresearch/fairseq/tree/main/examples/mbart) |
| 5 Nov 2019 | XLM-R (xlm-roberta-base) | Unsupervised Cross-lingual Representation Learning at Scale | [arXiv](https://arxiv.org/abs/1911.02116) [GitHub](https://github.com/pytorch/fairseq/tree/main/examples/xlmr) |
| 1 Nov 2019 | DialoGPT | DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation | [arXiv](https://arxiv.org/abs/1911.00536) [GitHub](https://github.com/microsoft/DialoGPT) |
| 2 Oct 2019 | DistilBERT | DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter | [arXiv](https://arxiv.org/abs/1910.01108) [GitHub](https://github.com/huggingface/transformers/tree/main/src/transformers/models/distilbert) |
| 26 Sep 2019 | ALBERT | ALBERT: A Lite BERT for Self-supervised Learning of Language Representations | [arXiv](https://arxiv.org/abs/1909.11942) [GitHub](https://github.com/google-research/albert) |
| 26 July 2019 | RoBERTa (RoBERTa-Large, RoBERTa-Base) | RoBERTa: A Robustly Optimized BERT Pretraining Approach | [arXiv](https://arxiv.org/abs/1907.11692) [GitHub](https://github.com/pytorch/fairseq/tree/main/examples/roberta) |
| 19 June 2019 | XLNet | XLNet: Generalized Autoregressive Pretraining for Language Understanding | [arXiv](https://arxiv.org/abs/1906.08237) [GitHub](https://github.com/zihangdai/xlnet) |
| Feb 2019 | GPT2 (GPT2, GPT2-M, GPT2-L) | Language Models are Unsupervised Multitask Learners | [Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [GitHub](https://github.com/openai/gpt-2) |
| 11 Oct 2018 | BERT-Base,Large | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | [arXiv](https://arxiv.org/abs/1810.04805) [GitHub](https://github.com/google-research/bert) |
| 12 June 2017 | Transformer | Attention Is All You Need | [arXiv](https://arxiv.org/abs/1706.03762) [GitHub](https://github.com/tensorflow/tensor2tensor) |
| July 2002 | BLEU | BLEU: A Method for Automatic Evaluation of Machine Translation | [Paper](https://aclanthology.org/P02-1040.pdf) |


# PEFT Methods in Vision Models


## Models and Papers

| Year | Model | Paper Title | Links |
|------|-------|-------------|-------|
| 7 Mar 2025 | FaceT-B, PLFace | Efficient Fine-tuning Strategies for Enhancing Face Recognition Performance in Challenging Scenarios |[Paper](https://ieeexplore.ieee.org/abstract/document/10887682) |
| 5 Apr 2023 | SAM | Segment Anything | [arXiv](https://arxiv.org/abs/2304.02643) [GitHub](https://github.com/facebookresearch/segment-anything) |
| 24 Mar 2023 | Point-PEFT | Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models | [Paper](https://ojs.aaai.org/index.php/AAAI/article/view/28323) |
| 16 Mar 2023 | ESM2 | Evolutionary-scale prediction of atomic protein structure with a language model | [Paper](https://www.science.org/doi/10.1126/science.ade2574) [GitHub](https://github.com/facebookresearch/esm) |
| 20 Dec 2022 | DiT | Scalable Diffusion Models with Transformers | [arXiv](https://arxiv.org/abs/2212.09748) [GitHub](https://github.com/facebookresearch/DiT) |
| 21 Feb 2022 | PointM2AE | Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training | [Paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/ad1d7a4df30a9c0c46b387815a774a84-Abstract-Conference.html) |
| 23 Mar 2022 | VideoMAE | VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training | [arXiv](https://arxiv.org/abs/2203.12602) [GitHub](https://github.com/MCG-NJU/VideoMAE) |
| 16 Mar 2022 | CodeReviewer | Automating Code Review Activities by Large Language Models | [arXiv](https://arxiv.org/abs/2203.09095) |
| 13 Mar 2022 | Point-MAE | Point-MAE: Masked Autoencoders for 3D Point Cloud Self-supervised Learning | [arXiv](https://arxiv.org/abs/2203.06604) [GitHub](https://github.com/Pang-Yatian/Point-MAE) |
| 11 Jan 2022 | ConvNeXt | A ConvNet for the 2020s | [arXiv](https://arxiv.org/abs/2201.03545) [GitHub](https://github.com/facebookresearch/ConvNeXt) |
| 29 Nov 2021 | Point-BERT | Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling | [arXiv](https://arxiv.org/abs/2111.14819) [GitHub](https://github.com/lulutang0608/Point-BERT) |
| 11 Nov 2021 | MAE | Masked Autoencoders Are Scalable Vision Learners | [arXiv](https://arxiv.org/abs/2111.06377) [GitHub](https://github.com/facebookresearch/mae) |
| 24 Jun 2021 | Swin Video Transformer | Video Swin Transformer | [arXiv](https://arxiv.org/abs/2106.13230) [GitHub](https://github.com/SwinTransformer/Video-Swin-Transformer) |
| 31 May 2021 | Segformer | SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers | [arXiv](https://arxiv.org/abs/2105.15203) [GitHub](https://github.com/NVlabs/SegFormer) |
| 18 Jul 2021 | AS-MLP | AS-MLP: An Axial Shifted MLP Architecture for Vision | [arXiv](https://arxiv.org/abs/2107.08391) |
| 25 Mar 2021 | Swin Transformer (Swin-L, Swin-B) | Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | [arXiv](https://arxiv.org/abs/2103.14030) [GitHub](https://github.com/microsoft/Swin-Transformer) |
| 26 Feb 2021 | CLIP | Learning Transferable Visual Models From Natural Language Supervision | [arXiv](https://arxiv.org/abs/2103.00020) [GitHub](https://github.com/openai/CLIP) |
| 24 Feb 2021 | PVT | Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions | [arXiv](https://arxiv.org/abs/2102.12122) [GitHub](https://github.com/whai362/PVT) |
| 11 Feb 2021 | NFNets | High-Performance Large-Scale Image Recognition Without Normalization | [arXiv](https://arxiv.org/abs/2102.06171) [GitHub](https://github.com/deepmind/deepmind-research/tree/master/nfnets) |
| 31 Dec 2020 | SETR | Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers | [arXiv](https://arxiv.org/abs/2012.15840) [GitHub](https://github.com/fudan-zvg/SETR) |
| 23 Dec 2020 | DeiT | Training data-efficient image transformers & distillation through attention | [arXiv](https://arxiv.org/abs/2012.12877) [GitHub](https://github.com/facebookresearch/deit) |
| 22 Oct 2020 | ViT (ViT-B/16, ViT-S) | An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | [arXiv](https://arxiv.org/abs/2010.11929) [GitHub](https://github.com/google-research/vision_transformer) |
| 7 Mar 2020 | RegNetX | Designing Network Design Spaces | [arXiv](https://arxiv.org/abs/2003.13678) [GitHub](https://github.com/facebookresearch/pycls) |
| 13 Feb 2020 | SimCLR | A Simple Framework for Contrastive Learning of Visual Representations | [arXiv](https://arxiv.org/abs/2002.05709) [GitHub](https://github.com/google-research/simclr) |
| 30 Jun 2018 | SELDnet | Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks | [arXiv](https://arxiv.org/abs/1807.00129) |
| 17 Mar 2017 | ProtoNet | Prototypical Networks for Few-shot Learning | [arXiv](https://arxiv.org/abs/1703.05175) [GitHub](https://github.com/jakesnell/prototypical-networks) |
| 23 May 2016 | WideResNet | Wide Residual Networks | [arXiv](https://arxiv.org/abs/1605.07146) [GitHub](https://github.com/szagoruyko/wide-residual-networks) |
| 10 Dec 2015 | ResNet (ResNet-50, ResNet-101) | Deep Residual Learning for Image Recognition | [arXiv](https://arxiv.org/abs/1512.03385) [GitHub](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py) |
| 4 Sep 2014 | VGG (VGG16, VGG19) | Very Deep Convolutional Networks for Large-Scale Image Recognition | [arXiv](https://arxiv.org/abs/1409.1556) [GitHub](https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py) |





# üöÄ PEFT Methods in NLP Tasks Datasets

| Dataset        | Task Type                                          | Dataset Size         | Dataset Link |
|----------------|----------------------------------------------------|-----------------------|--------------|
| 20 Newsgroups  | Text Classification                                | 20K                  |[Link](https://archive.ics.uci.edu/dataset/113/twenty+newsgroups)
| CoNLL03        | Named Entity Recognition                           | 2302 articles        |[Link](http://arxiv.org/abs/cs/0306050)|
| CoNLL04        | Relation Extraction Tasks                          | 1,437 sentences      |[Link](https://aclanthology.org/W04-2401/)              |
| SST-2          | Sentiment Classification                           | 215,154              |[Link](https://aclanthology.org/D13-1170/)             |
| MRPC           | Paraphrase Detection                               | 5,800                |[Link](https://aclanthology.org/I05-5002/)              |
| ACE2005        | Information Extraction (IE)                        | 599 documents        |[Link](https://github.com/ualbertalib/metadata/blob/master/metadata-wrangling/ldc_metadata/ual_ldc_marc.mrc)              |
| WMT            | Machine Translation                                | N/A                  |[Link](https://aclanthology.org/2022.wmt-1.1.pdf)              |
| MPQA           | Opinion Mining                                     | 535 (articles)       |[Link](https://www.cs.cornell.edu/home/cardie/papers/lre05withappendix.pdf)              |
| Common Crawl   | Web Scraping, NLP Pretraining                      | 386 TiB              |[Link](https://commoncrawl.org/)              |
| i2b2 2010 RE   | Relation Extraction in Clinical Text               | 877 documents        |[Link](https://pmc.ncbi.nlm.nih.gov/articles/PMC3168320/)              |
| Semeval-2010   | Semantic Role Labeling, WSD, Relation Extraction   | 284                  |[Link](https://aclanthology.org/S10-1004/)              |
| COPA           | Commonsense Causal Reasoning                       | 1000 questions       |[Link](https://aclanthology.org/S12-1052/)              |
| IMDb           | Sentiment Classification                           | 50k reviews          |[Link](https://aclanthology.org/P11-1015/)              |
| SBU            | Image Captioning                                   | 1 million images     |[Link](https://ieeexplore.ieee.org/document/8953407)              |
| WebQ           | Question Answering                                 | 6,642                |[Link](https://fcon_1000.projects.nitrc.org/indi/abide/abide_I.html)              |
| OntoNotes      | NER, Coreference Resolution                        | 2.9453M              |[Link](https://aclanthology.org/D13-1160/)              |
| DUT            | Salient Object Detection                           | 5,168 images         |[Link](https://ieeexplore.ieee.org/document/6619251)              |
| AG News        | Text Classification                                | 1M                   |[Link](https://arxiv.org/abs/1509.01626)              |
| Yelp           | Sentiment Classification                           | 700,000              |[Link](https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset)             |
| VQA v2.0       | Visual Question Answering                          | 265,016              |[Link](https://arxiv.org/pdf/1509.01626)              |
| TriviaQA       | Question Answering                                 | 650K                 |[Link](https://arxiv.org/html/2406.11811v1)              |
| SQuAD          | Question Answering                                 | 100K                 |[Link](https://aclanthology.org/D16-1264/)              |
| BoolQ          | Boolean Question Answering                         | 15,942               |[Link](https://github.com/google-research-datasets/boolean-questions)             |
| YELP-Polarity   | Sentiment Analysis (Binary Classification)       | 560,000              |[Link](https://paperswithcode.com/dataset/yelp-review-polarity)              |
| CNN/DailyMail   | Abstractive Summarization                         | 313k                 |[Link](https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail)              |
| Yelp            | Sentiment Classification                         | 700,000              |[Link](https://www.kaggle.com/datasets/yelp-dataset/yelp-dataset)              |
| WMT 16 en-ro    | Machine Translation (English to Romanian and vice versa) | 2 million sentences  |[Link](https://arxiv.org/abs/1606.02891)              |
| MAWPS           | Math Word Problem Solving                         | 2,373 problems        |[Link](https://aclanthology.org/N16-1136/)              |
| MIMIC-III MP    | Medical Predictive Modeling                       | 60,000 ICU stays      |[Link](https://pmc.ncbi.nlm.nih.gov/articles/PMC4878278/)              |
| WMT 16 en-ro    | Machine Translation (English to Romanian and vice versa) | 2 million sentences  |[Link](https://arxiv.org/abs/1606.02891)              |
| E2E             | Data to Text Generation                           | N/A                  |[Link](https://paperswithcode.com/sota/data-to-text-generation-on-e2e)              |
| MNLI            | Natural Language Inference                        | 433k                 |[Link](https://paperswithcode.com/dataset/multinli)              |
| WebNLG          | Natural Language Generation                       | 13,211 triples        |[Link](https://synalp.gitlabpages.inria.fr/webnlg-challenge/files/2020.webnlg-papers.7.pdf)              |
| STS-B           | Semantic Textual Similarity                       | 8,628 pairs           |[Link](https://aclanthology.org/S17-2001/)              |
| HS              | Hate Speech Detection                             | 25,000 tweets         |[Link](https://github.com/t-davidson/hate-speech-and-offensive-language)              |
| WikiSQL         | Text-to-SQL Generation                            | 80,654                |[Link](https://paperswithcode.com/dataset/wikisql)              |
| ARC             | Multiple Choice Question Answering               | 7,787                |[Link](https://arxiv.org/abs/1803.05457)              |
| GLUE            | Natural Language Understanding                    | N/A                  |[Link](https://arxiv.org/abs/1804.07461)              |
| OBQA            | Commonsense Question Answering                    | 5,957 questions       |[Link](https://aclanthology.org/D18-1260/)              |
| ARC-Easy        | Multiple-choice Question Answering                | 5,197                |[Link](https://arxiv.org/abs/1803.05457)              |
| ARC-Challenge   | Multiple-choice Question Answering                | 2,590                |[Link](https://arxiv.org/abs/1803.05457)              |
| MultiRC                | Multi-Sentence Reading Comprehension                | 10,000 questions          |[Link](https://aclanthology.org/N18-1023/)              |
| XSum                   | Abstractive Summarization                           | 226,711                   |[Link](https://www.creativefabrica.com/product/narayana/)              |
| CoLA                   | Sentence Acceptability                              | 10,657 sentences          |[Link](https://github.com/nyu-mll/CoLA/tree/master)              |
| SciTail dataset        | Textual Entailment                                  | 27,026 examples           |[Link](https://github.com/tensorflow/datasets/blob/master/docs/catalog/sci_tail.md)              |
| SQuAD 2.0              | Question Answering                                  | 150,000+ question-answer pairs |[Link](https://arxiv.org/abs/1606.05250)         |
| PIQA                   | Multiple Choice Question Answering                  | 19K                       |[Link](https://arxiv.org/abs/1911.11641)              |
| Winogrande             | Commonsense Reasoning                               | 44K                       |[Link](https://arxiv.org/abs/1907.10641)              |
| OSCAR                  | Pre-training Language Models                        | 50K                       |[Link](https://oscar-project.org/publication/2019/clmc7/asynchronous/)              |
| SuperGLUE              | Natural Language Understanding (NLU)                | N/A                       |[Link](https://arxiv.org/abs/1804.07461)              |
| codeSearchNet          | --                                                  | 2 million                 |[Link](https://arxiv.org/abs/1909.09436)              |
| AmazonQA/ProductQA     | Question Answering (QA)                             | 923K questions, 3.6M answers |[Link](https://arxiv.org/abs/1908.04364)           |
| commonsenseQA (CSQA)   | Question Answering                                  | 12,247 questions          |[Link](https://aclanthology.org/N19-1421/)              |
| SAMSum                 | Dialogue Summarization                              | 16,369                    |[Link](https://aclanthology.org/D19-5409/)              |
| Word-in-Context (WiC)  | Evaluate Context-Sensitive Meaning                  | 7,466                     |[Link](https://aclanthology.org/N19-1128/)              |
| Hyperpartisan          | Binary Classification (Hyperpartisan vs. Non-Hyperpartisan) | 754,000 articles     |[Link](https://aclanthology.org/S19-2145/)              |
| PAWS                   | Paraphrase Identification                           | 108,463                   |[Link](https://aclanthology.org/N19-1131/)              |
| WoW                    | Knowledge-driven Dialogue Generation                | N/A                       |[Link](https://openreview.net/forum?id=r1l73iRqKm)              |
| CosmosQA               | Commonsense Reading Comprehension                   | 35.6K questions           |[Link](https://aclanthology.org/D19-1243/)              |
| HellaSwag              | Commonsense Reasoning / Sentence Completion         | N/A                       |[Link](https://aclanthology.org/P19-1472/)              |
| MMLU                         | Multiple-Choice QA                            | 15,000+ questions across 57 subjects |[Link](https://arxiv.org/abs/2009.03300)              |
| The Pile                     | Language Modeling                             | 825GB                             |[Link](https://arxiv.org/abs/2101.00027)              |
| WikiTableText                | Data-to-Text / QA                             | 100M+ tokens                      |[Link](https://arxiv.org/abs/1609.07843)              |
| DART                         | Data-to-Text                                  | 82,191 examples                   |[Link](https://aclanthology.org/2021.naacl-main.37/)              |
| C4                           | Language Modeling / Pre-training              | 806.87 GiB                        |[Link](https://arxiv.org/abs/1910.10683)              |
| LibriLight                   | Self-supervised Speech                        | 60,000 hours                      |[Link](https://awwa.onlinelibrary.wiley.com/doi/full/10.1002/aws2.1179)              |
| ANLI                         | Natural Language Inference                    | 162,865 examples                  |[Link](https://csrc.nist.gov/pubs/ai/100/2/e2023/final)              |
| COMMONGEN                    | Text Generation                               | 79K descriptions over 35K concept sets |[Link](https://aclanthology.org/2020.findings-emnlp.165/)           |
| GSM8K (Grade School Math 8K) | Mathematical Problem Solving                  | 8,500                             |[Link](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/gsm8k/README.md)              |
| MATH                         | Mathematical Problem Solving                  | 12,500                            |[Link](https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/gsm8k/README.md)              |
| TruthfulQA                   | Question Answering, Text Generation           | 817                               |[Link](https://arxiv.org/abs/2109.07958)              |
| Therapeutics Data Commons benchmark | AI-driven drug discovery            | 66 AI-ready datasets              |[Link](https://pubmed.ncbi.nlm.nih.gov/34305919/)              |
| SVAMP                        | Math Word Problem Solving                     | 1,000 problems                    |[Link](https://github.com/arkilpatel/SVAMP)              |
| CUB-200-2011   | Fine-grained Image Classification                  | 11,788 images              |[Link](https://authors.library.caltech.edu/records/cvm3y-5hh21)              |
| EL EVATER      | Multimodal Entity Linking, Visual Attribute Recognition | 10,000 instances (text-image pairs) |[Link](https://w3.jamstec.go.jp/apl/j/members/sasaki/ofes_publication.html)     |
| TB-1k          | Medical Image Classification                       | 1,000K                     |[Link](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)              |
| ro-en          | --                                                 | 614,318 sentences          |              |
| de-en          | --                                                 | 4,554,053 sentences        |              |
| Ha‚ÄìEn          | --                                                 | 50,000 sentences           |              |
| (De-En)        | --                                                 | N/A                        |              |
| Fr‚ÄìDe          | --                                                 | N/A                        |              |
| Fr‚ÄìEs          | --                                                 | N/A                        |              |
| Flan v2                  | Instruction tuning for NLP tasks                   | 1,800+                              |[Link](https://jasonwei20.github.io/files/FLAN%20talk%20external.pdf)              |
| BBH                      | Question-Answering                                 | 6,511                               |[Link](https://arxiv.org/abs/2210.09261)              |
| Pix2Struc                | Visual Language Understanding                      | N/A                                 |[Link](https://arxiv.org/abs/2210.03347)              |
| M2D2                     | Text                                               | 8.5B tokens across 145 domains      |[Link](https://aclanthology.org/2022.emnlp-main.63/)              |
| Alpaca                   | Instruction Following                              | 52K                                 |[Link](https://crfm.stanford.edu/2023/03/13/alpaca.html)              |
| GPT-4 Alpaca             | Instruction-following fine-tuning                  | 52K                                 |[Link](https://github.com/tatsu-lab/stanford_alpaca)              |
| Dolly                    | Instruction-Following NLP                          | 15K prompts                         |[Link](https://aclanthology.org/N12-4001/)              |
| Orca                     | Instruction-Following, Reasoning                   | 1.6 million                         |[Link](https://arxiv.org/abs/2306.02707)              |
| GPT-4-Turbo              | --                                                 | 128k                                |N/A             |
| AI4Bharat Naamapadam     | NER on Indian languages                            | 400k sentences                      |[Link](https://aclanthology.org/2023.acl-long.582/)              |
| AmericasNLI              | Natural Language Inference                         | N/A                                 |N/A              |
| SIQA                     | Text                                               | 33,410                              |N/A              |
| TREC                     | Question Classification                            | 4,500                               |[Link](https://aclanthology.org/C02-1150/)              |
| ScienceQ                 | Science Question Answering                         | N/A                                 |N/A              |
| Wikitext2                | Language Modeling                                  | 2M                                  |[Link](https://arxiv.org/abs/1609.07843)              |
| Penn Treebank            | Language Modeling                                  | 1M                                  |N/A              |
| VQA v2.0                 | Visual Question Answering                          | 265,016                             |[Link](https://arxiv.org/abs/1612.00837)              |
| VisDA-C                  | Domain Adaptation for Image Classification         | 280k images                         |[Link](https://arxiv.org/abs/1710.06924)              |
| ImageNet-Sketch          | Image Classification                               | 50,889 images                       |[Link](https://papers.nips.cc/paper_files/paper/2019/hash/3eefceb8087e964f89c2d59e8a249915-Abstract.html)              |
| ImageNet-A               | Image Classification                               | 7,500 images                        |[Link](https://paperswithcode.com/dataset/imagenet-a)              |
| DocVQA                   | Visual Question Answering (VQA) on documents       | 50,000 questions, 12,767 images     |[Link](https://arxiv.org/abs/2007.00398)              |
| IWSLT                    | --                                                 | N/A                                 |N/A              |
| FGVC                     | Fine-grained object classification                 | 10,200 images of aircraft           |[Link](https://arxiv.org/abs/1306.5151)              |
| OPUS-100                         | Multilingual Machine Translation (text-to-text)                      | 55 million parallel sentences           |[Link](https://www.researchgate.net/publication/361063314_OneAligner_Zero-shot_Cross-lingual_Transfer_with_One_Rich-Resource_Language_Pair_for_Low-Resource_Sentence_Retrieval)              |
| MS COCO dataset                  | Object Detection, Instance Segmentation, Panoptic Segmentation, Image Captioning, Keypoint Detection | 118K images / 5K images / 41K images |[Link](https://arxiv.org/abs/1405.0312)              |
| COCO Stuff                       | Semantic Segmentation, Panoptic Segmentation, Scene Understanding    | 164,000 images                          |[Link](https://github.com/nightrome/cocostuff)              |
| PASCAL VOC                       | Object Detection, Semantic Segmentation, Image Classification, Action Recognition, Person Layout | VOC 2012: 11,530 images; Object instances: 27,450; Segmentation: 2,913 images with pixel masks |[Link](http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.html)              |
| MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge | Multi-organ Segmentation (13 abdominal organs)                     | 30 CT scans / 20 CT scans               |[Link](https://paperswithcode.com/dataset/miccai-2015-multi-atlas-abdomen-labeling)              |
| PASCAL Context                   | Object Detection/Segmentation, Semantic Segmentation, Scene Parsing  | 10,103 images with 65,937 labeled objects; Classes: 459 |[Link](https://openaccess.thecvf.com/content_cvpr_2014/html/Mottaghi_The_Role_of_2014_CVPR_paper.html)              |
| Pascal Context-59               | Semantic Segmentation, Scene Parsing                                 | 10,103 images with dense labels for 59 classes |       |
| Pascal Context-459              | --                                                                    | N/A                                     |              |
| ADE20K 150           | Semantic Segmentation, Scene Parsing, Instance Segmentation (optional) | 25,000 images                    |              |
| ADE20K-847           | Semantic Segmentation, Scene Parsing, Instance Segmentation (optional) | 25,000 images                    |              |
| CIFAR-10             | Image Classification                                | 60,000 images                         |              |
| CIFAR-100            | Image Classification                                | 60,000 images                         |              |
| CIFAR-10-LT (Long Tail) | Image Classification                            | 60,000 images                         |              |
| ImageNet1K           | Image Classification and Localization               | 1,431,167 images                      |              |
| ImageNet100          | Image Classification                                | 131,689 images                        |              |
| ORBIT                | Few-shot learning for teachable object recognition  | 2,687 videos                          |              |
| ScanObjectNN         | 3D Object Classification                            | 2,902 object instances                |              |
| ModelNet40           | 3D Shape Classification                             | 12,311 3D objects / 2,468 models      |              |
| RESISC45             | Remote Sensing Scene Classification                 | 31,500 RGB images                     |              |
| CLEVR                | Visual Question Answering (VQA), Compositional Reasoning | 100,000 images / 1 million questions |              |
| DepthTrack           | RGB-D Single Object Tracking                        | 150 sequences (100,000+ frames)       |              |
| IconQA             | Visual Question Answering                        | N/A                               |              |
| ImageNet-R         | Image Classification                             | 30,000 images                     |              |
| Pix2Struc          | Visual Language Understanding                    | N/A                               |              |
| M2D2 (2022)        | Multi-task NLP (NER, RE, QA, etc.)               | 8.5B tokens across 145 domains    |              |
| Vizwiz             | Visual Question Answering                        | N/A                               |              |
| Flickr30k          | Image Captioning                                 | 31,783                            |              |
| OKVQA              | Visual Question Answering                        | 14,055                            |              |
| OCR-VQA            | Optical Character Recognition                    | N/A                               |              |
| LibriSpeech        | Automatic Speech Recognition                     | 1,000 hours                       |              |
| M2D2 (2022 again)  | Multi-task NLP (NER, RE, QA, etc.)               | 8.5B tokens across 145 domains    |              |
| ImageNet-Sketch    | Image Classification                             | 50,889 images                     |              |
| ImageNet-A         | Image Classification                             | 7,500 images                      |              |
| ImageNet-C         | Image Classification under Corruption            | 3,750,000 corrupted images        |              |
| VisDA-C            | Domain Adaptation for Image Classification       | 280K images                       |              |
| DomainNet-126      | Domain Adaptation for Image Classification       | 586,575 images                    |              |
| SBU                | Image Captioning                                 | 1 million images                  |              |
| OxfordPets         | Fine-Grained Classification & Semantic Segmentation | 7,390 images                   |              |
| DUT                | Salient Object Detection                         | 5,168 images                      |              |
| DTD                   | Texture Classification                                | 5,640 images                          |              |
| Food101               | Food Image Classification                             | 101,000 images                        |              |
| Kinetics-400          | Action Recognition, Video Classification              | 650k video clips                      |              |
| Something-Something-v2| Temporal Action Recognition                           | 220,847 videos                        |              |
| EuroSAT               | Land Use/Land Cover Classification                    | 27,000 images                         |              |
| iNaturalist 2018      | Fine-Grained Species Classification                   | 611,753 images                        |              |
| IMD20                 | --                                                    | 35,000 images                         |              |
| COD10K                | Camouflaged Object Detection                          | 10,000 images                         |              |
| CAMO                  | Camouflaged Object Segmentation                       | 1,250 images                          |              |
| ISTD                  | Shadow Detection & Removal                            | N/A                                   |              |
| UCF101                | Action Recognition, Video Classification              | 13,320 videos (7.2 GB)                |              |
| SUN397                | Scene Classification                                  | 108,753 images                        |              |
| ABIDE                 | Neuroimaging Classification (ASD vs. Controls)        | 1,112 subjects                        |              |
| Places-LT             | Long-Tailed Scene Classification                      | 62,500 images                         |              |
| CASIA                 | --                                                    | N/A                                   |              |
| IMD20                 | --                                                    | N/A                                   |              |
| CUHK                 | --                                                    | N/A                                   |              |
| CHAMELEON (Confused)  | --                                                    | N/A                                   |              |
| Kinetics-700          | --                                                    | N/A                                   |              |
| DF-20M mini           | --                                                    | N/A                                   |              |
| RIGA+ (Not found)     | --                                                    | 55                                    |              |
| SCGM (Not found)      | --                                                    | N/A                                   |              |
| VQA v2.0              | Visual Question Answering                             | 265,016                               |              |


